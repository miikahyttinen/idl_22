{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse file nums to dicts\n",
    "annotation_file_path = os.getcwd() + '/../data/annotations'\n",
    "annotation_ids = []\n",
    "for file in os.listdir(annotation_file_path):\n",
    "    filename = os.fsdecode(file)\n",
    "    file_path = os.path.join(annotation_file_path, filename)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            idx = int(line.replace('\\n', ''))\n",
    "            annotation_ids.append(idx)\n",
    "            \n",
    "annotation_ids = list(set(annotation_ids))\n",
    "train_ids, dev_ids = train_test_split(annotation_ids, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE_TRAIN = 128\n",
    "BATCH_SIZE_DEV = 16\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 14\n",
    "DATA_DIR = os.getcwd() + '/../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class ProjectDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotation_folder, selected_keys=None, transform=None, target_transform=None):\n",
    "        data = defaultdict(list)\n",
    "        self.data = []\n",
    "        self.classes = []\n",
    "        \n",
    "        for file in os.listdir(annotation_folder):\n",
    "            filename = os.fsdecode(file)\n",
    "            cls = filename.split('.')[0]\n",
    "            self.classes.append(cls)\n",
    "            file_path = os.path.join(annotation_folder, filename)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    idx = int(line.replace('\\n', ''))\n",
    "                    data[idx].append(cls)\n",
    "        \n",
    "        for key, val in data.items():\n",
    "            if not selected_keys or key in selected_keys:\n",
    "                filename = f'im{key}.jpg'\n",
    "                labels = torch.Tensor([x in val for x in self.classes])\n",
    "                self.data.append({'file': filename, 'labels': labels})\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = self.data[item]\n",
    "        labels = data['labels']\n",
    "        image_path = os.path.join(self.image_folder, data['file'])\n",
    "        image = torchvision.io.read_image(image_path, torchvision.io.ImageReadMode.RGB)\n",
    "        image = image.type(torch.FloatTensor)\n",
    "        image = torch.div(image, 255)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "            \n",
    "        return image, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      #transforms.RandomPerspective(distortion_scale=0.2),\n",
    "                                      #transforms.ColorJitter(),\n",
    "                                      #transforms.ToTensor()\n",
    "                                     ])\n",
    "dev_transform = transforms.Compose([])\n",
    "\n",
    "annotation_path = DATA_DIR + 'annotations'\n",
    "image_path = DATA_DIR + 'images'\n",
    "train_set = ProjectDataset(image_path, annotation_path, selected_keys=train_ids, transform=train_transform)\n",
    "dev_set = ProjectDataset(image_path, annotation_path, selected_keys=dev_ids, transform=dev_transform)\n",
    "# test_set  = datasets.ImageFolder(DATA_DIR % 'test',  transform=test_transform)\n",
    "\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_set, batch_size=BATCH_SIZE_DEV, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(32768, NUM_CLASSES)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_metrics(targets, preds):\n",
    "    false_neg = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    true_pos = 0\n",
    "    preds_bin = (preds > 0.5).float()\n",
    "    \n",
    "    for t_labels, p_labels in zip(targets, preds_bin):\n",
    "            for t, p in zip(t_labels, p_labels):\n",
    "                if(t == 1 and p == 0):\n",
    "                    false_neg = false_neg + 1\n",
    "                if(t == 0 and p == 1):\n",
    "                    false_pos = false_pos + 1\n",
    "                if(t == 0 and p == 0):\n",
    "                    true_neg = true_neg + 1\n",
    "                if(t == 1 and p == 1):\n",
    "                    true_pos = true_pos + 1\n",
    "    recall = true_pos / (true_pos + false_pos)\n",
    "    precision = true_pos / (true_pos + false_neg)\n",
    "    f1_score = 2 * (precision*recall / (precision+recall))\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "#--- set up ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('using gpu')\n",
    "else:\n",
    "    print('using cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# OPTIMIZERS & REGURALIZATION\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=0.05)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "#--- TRAINING ---\n",
    "\n",
    "previous_train_loss = 1000000\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "                \n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        total += len(target)\n",
    "        train_correct += torch.sum((pred > 0.5).float() == target) / torch.sum(target)\n",
    "        \n",
    "        precision, recall, f1_score = calc_batch_metrics(target, pred)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('\\r', 'Training: Epoch %d - Batch %d/%d: Loss: %.4f | Acc: %.3f%% (%d/%d)' % \n",
    "              (epoch, batch_num, len(train_loader), train_loss / (batch_num + 1), \n",
    "               100. * train_correct / total, train_correct, total), end='')\n",
    "        print('---')\n",
    "        print('Preicision: ' , precision)\n",
    "        print('Recall: ' , recall) \n",
    "        print('F1 Score: ' , f1_score)\n",
    "        print('---')\n",
    "    \n",
    "    #print()\n",
    "    #dev_loss = 0\n",
    "    #dev_total = 0\n",
    "    #dev_correct = 0\n",
    "    \n",
    "    #for batch_num, (data, target) in enumerate(dev_loader):\n",
    "    #    data, target = data.to(device), target.to(device)\n",
    "    #            \n",
    "        # Compute prediction error\n",
    "    #    pred = model(data)\n",
    "    #    loss = loss_function(pred, target)\n",
    "    #    dev_loss += loss.item()\n",
    "    #    \n",
    "    #    dev_total += len(target)\n",
    "    #    dev_correct += torch.sum((pred > 0.5).float() == target) / torch.sum(target)\n",
    "    #    \n",
    "    #    print('\\r', 'Dev: Epoch %d - Batch %d/%d: Loss: %.4f | Acc: %.3f%% (%d/%d)' % \n",
    "    #          (epoch, batch_num, len(dev_loader), dev_loss / (batch_num + 1), \n",
    "    #           100. * dev_correct / dev_total, dev_correct, dev_total), end='')\n",
    "    \n",
    "    print()\n",
    "    print(\"Epoch train loss: \" + str(train_loss))\n",
    "    print(\"Previous epoch dev loss: \" + str(previous_train_loss))\n",
    "    print()\n",
    "    \n",
    "    # EARLY STOPPING\n",
    "    if train_loss > previous_train_loss:\n",
    "        print('early stopping')\n",
    "        break\n",
    "        \n",
    "    previous_train_loss = train_loss\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(batch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 0/128: Loss: 0.3735 | Test Acc: 45.370% (7/16)\n",
      "Evaluating: Batch 1/128: Loss: 0.3507 | Test Acc: 42.040% (13/32)\n",
      "Evaluating: Batch 2/128: Loss: 0.4516 | Test Acc: 44.965% (21/48)\n",
      "Evaluating: Batch 3/128: Loss: 0.4188 | Test Acc: 42.101% (26/64)\n",
      "Evaluating: Batch 4/128: Loss: 0.4010 | Test Acc: 41.664% (33/80)\n",
      "Evaluating: Batch 5/128: Loss: 0.4159 | Test Acc: 40.876% (39/96)\n",
      "Evaluating: Batch 6/128: Loss: 0.4568 | Test Acc: 40.537% (45/112)\n",
      "Evaluating: Batch 7/128: Loss: 0.5004 | Test Acc: 39.823% (50/128)\n",
      "Evaluating: Batch 8/128: Loss: 0.4926 | Test Acc: 39.401% (56/144)\n",
      "Evaluating: Batch 9/128: Loss: 0.4829 | Test Acc: 39.482% (63/160)\n",
      "Evaluating: Batch 10/128: Loss: 0.4777 | Test Acc: 39.233% (69/176)\n",
      "Evaluating: Batch 11/128: Loss: 0.4782 | Test Acc: 38.958% (74/192)\n",
      "Evaluating: Batch 12/128: Loss: 0.4841 | Test Acc: 38.891% (80/208)\n",
      "Evaluating: Batch 13/128: Loss: 0.4933 | Test Acc: 39.131% (87/224)\n",
      "Evaluating: Batch 14/128: Loss: 0.4823 | Test Acc: 38.997% (93/240)\n",
      "Evaluating: Batch 15/128: Loss: 0.4834 | Test Acc: 38.523% (98/256)\n",
      "Evaluating: Batch 16/128: Loss: 0.5211 | Test Acc: 38.844% (105/272)\n",
      "Evaluating: Batch 17/128: Loss: 0.5115 | Test Acc: 38.678% (111/288)\n",
      "Evaluating: Batch 18/128: Loss: 0.5143 | Test Acc: 38.992% (118/304)\n",
      "Evaluating: Batch 19/128: Loss: 0.5119 | Test Acc: 39.100% (125/320)\n",
      "Evaluating: Batch 20/128: Loss: 0.5046 | Test Acc: 39.290% (132/336)\n",
      "Evaluating: Batch 21/128: Loss: 0.5075 | Test Acc: 39.062% (137/352)\n",
      "Evaluating: Batch 22/128: Loss: 0.5027 | Test Acc: 39.219% (144/368)\n",
      "Evaluating: Batch 23/128: Loss: 0.5073 | Test Acc: 39.336% (151/384)\n",
      "Evaluating: Batch 24/128: Loss: 0.4985 | Test Acc: 39.396% (157/400)\n",
      "Evaluating: Batch 25/128: Loss: 0.4959 | Test Acc: 39.385% (163/416)\n",
      "Evaluating: Batch 26/128: Loss: 0.4955 | Test Acc: 39.309% (169/432)\n",
      "Evaluating: Batch 27/128: Loss: 0.4939 | Test Acc: 39.136% (175/448)\n",
      "Evaluating: Batch 28/128: Loss: 0.4942 | Test Acc: 39.250% (182/464)\n",
      "Evaluating: Batch 29/128: Loss: 0.4924 | Test Acc: 39.100% (187/480)\n",
      "Evaluating: Batch 30/128: Loss: 0.4927 | Test Acc: 39.073% (193/496)\n",
      "Evaluating: Batch 31/128: Loss: 0.4883 | Test Acc: 39.206% (200/512)\n",
      "Evaluating: Batch 32/128: Loss: 0.4819 | Test Acc: 39.364% (207/528)\n",
      "Evaluating: Batch 33/128: Loss: 0.4770 | Test Acc: 39.419% (214/544)\n",
      "Evaluating: Batch 34/128: Loss: 0.4812 | Test Acc: 39.296% (220/560)\n",
      "Evaluating: Batch 35/128: Loss: 0.4914 | Test Acc: 39.050% (224/576)\n",
      "Evaluating: Batch 36/128: Loss: 0.4884 | Test Acc: 39.159% (231/592)\n",
      "Evaluating: Batch 37/128: Loss: 0.4822 | Test Acc: 39.009% (237/608)\n",
      "Evaluating: Batch 38/128: Loss: 0.4798 | Test Acc: 39.136% (244/624)\n",
      "Evaluating: Batch 39/128: Loss: 0.4870 | Test Acc: 39.095% (250/640)\n",
      "Evaluating: Batch 40/128: Loss: 0.5023 | Test Acc: 39.071% (256/656)\n",
      "Evaluating: Batch 41/128: Loss: 0.4971 | Test Acc: 38.920% (261/672)\n",
      "Evaluating: Batch 42/128: Loss: 0.4973 | Test Acc: 38.931% (267/688)\n",
      "Evaluating: Batch 43/128: Loss: 0.4971 | Test Acc: 38.760% (272/704)\n",
      "Evaluating: Batch 44/128: Loss: 0.4988 | Test Acc: 38.741% (278/720)\n",
      "Evaluating: Batch 45/128: Loss: 0.4964 | Test Acc: 38.697% (284/736)\n",
      "Evaluating: Batch 46/128: Loss: 0.4986 | Test Acc: 38.680% (290/752)\n",
      "Evaluating: Batch 47/128: Loss: 0.4968 | Test Acc: 38.578% (296/768)\n",
      "Evaluating: Batch 48/128: Loss: 0.5005 | Test Acc: 38.536% (302/784)\n",
      "Evaluating: Batch 49/128: Loss: 0.4953 | Test Acc: 38.691% (309/800)\n",
      "Evaluating: Batch 50/128: Loss: 0.4933 | Test Acc: 38.527% (314/816)\n",
      "Evaluating: Batch 51/128: Loss: 0.4911 | Test Acc: 38.692% (321/832)\n",
      "Evaluating: Batch 52/128: Loss: 0.4958 | Test Acc: 38.662% (327/848)\n",
      "Evaluating: Batch 53/128: Loss: 0.4957 | Test Acc: 38.867% (335/864)\n",
      "Evaluating: Batch 54/128: Loss: 0.4925 | Test Acc: 38.784% (341/880)\n",
      "Evaluating: Batch 55/128: Loss: 0.4966 | Test Acc: 38.813% (347/896)\n",
      "Evaluating: Batch 56/128: Loss: 0.4963 | Test Acc: 38.849% (354/912)\n",
      "Evaluating: Batch 57/128: Loss: 0.5001 | Test Acc: 38.977% (361/928)\n",
      "Evaluating: Batch 58/128: Loss: 0.5048 | Test Acc: 39.036% (368/944)\n",
      "Evaluating: Batch 59/128: Loss: 0.5049 | Test Acc: 38.941% (373/960)\n",
      "Evaluating: Batch 60/128: Loss: 0.5023 | Test Acc: 38.914% (379/976)\n",
      "Evaluating: Batch 61/128: Loss: 0.5001 | Test Acc: 38.924% (386/992)\n",
      "Evaluating: Batch 62/128: Loss: 0.4982 | Test Acc: 38.983% (392/1008)\n",
      "Evaluating: Batch 63/128: Loss: 0.4982 | Test Acc: 38.928% (398/1024)\n",
      "Evaluating: Batch 64/128: Loss: 0.4952 | Test Acc: 39.161% (407/1040)\n",
      "Evaluating: Batch 65/128: Loss: 0.4994 | Test Acc: 39.183% (413/1056)\n",
      "Evaluating: Batch 66/128: Loss: 0.4979 | Test Acc: 39.185% (420/1072)\n",
      "Evaluating: Batch 67/128: Loss: 0.4962 | Test Acc: 39.143% (425/1088)\n",
      "Evaluating: Batch 68/128: Loss: 0.4942 | Test Acc: 39.081% (431/1104)\n",
      "Evaluating: Batch 69/128: Loss: 0.4941 | Test Acc: 39.034% (437/1120)\n",
      "Evaluating: Batch 70/128: Loss: 0.4930 | Test Acc: 38.941% (442/1136)\n",
      "Evaluating: Batch 71/128: Loss: 0.5005 | Test Acc: 38.933% (448/1152)\n",
      "Evaluating: Batch 72/128: Loss: 0.4981 | Test Acc: 39.074% (456/1168)\n",
      "Evaluating: Batch 73/128: Loss: 0.5016 | Test Acc: 39.232% (464/1184)\n",
      "Evaluating: Batch 74/128: Loss: 0.4997 | Test Acc: 39.176% (470/1200)\n",
      "Evaluating: Batch 75/128: Loss: 0.4994 | Test Acc: 39.101% (475/1216)\n",
      "Evaluating: Batch 76/128: Loss: 0.4969 | Test Acc: 39.246% (483/1232)\n",
      "Evaluating: Batch 77/128: Loss: 0.4993 | Test Acc: 39.172% (488/1248)\n",
      "Evaluating: Batch 78/128: Loss: 0.4984 | Test Acc: 39.254% (496/1264)\n",
      "Evaluating: Batch 79/128: Loss: 0.4991 | Test Acc: 39.257% (502/1280)\n",
      "Evaluating: Batch 80/128: Loss: 0.4995 | Test Acc: 39.224% (508/1296)\n",
      "Evaluating: Batch 81/128: Loss: 0.4999 | Test Acc: 39.187% (514/1312)\n",
      "Evaluating: Batch 82/128: Loss: 0.4988 | Test Acc: 39.201% (520/1328)\n",
      "Evaluating: Batch 83/128: Loss: 0.5005 | Test Acc: 39.139% (526/1344)\n",
      "Evaluating: Batch 84/128: Loss: 0.5014 | Test Acc: 39.127% (532/1360)\n",
      "Evaluating: Batch 85/128: Loss: 0.4989 | Test Acc: 39.213% (539/1376)\n",
      "Evaluating: Batch 86/128: Loss: 0.5019 | Test Acc: 39.171% (545/1392)\n",
      "Evaluating: Batch 87/128: Loss: 0.5021 | Test Acc: 39.141% (551/1408)\n",
      "Evaluating: Batch 88/128: Loss: 0.5001 | Test Acc: 39.153% (557/1424)\n",
      "Evaluating: Batch 89/128: Loss: 0.5009 | Test Acc: 39.124% (563/1440)\n",
      "Evaluating: Batch 90/128: Loss: 0.5004 | Test Acc: 39.032% (568/1456)\n",
      "Evaluating: Batch 91/128: Loss: 0.4989 | Test Acc: 39.047% (574/1472)\n",
      "Evaluating: Batch 92/128: Loss: 0.4991 | Test Acc: 39.144% (582/1488)\n",
      "Evaluating: Batch 93/128: Loss: 0.4986 | Test Acc: 39.234% (590/1504)\n",
      "Evaluating: Batch 94/128: Loss: 0.5026 | Test Acc: 39.208% (595/1520)\n",
      "Evaluating: Batch 95/128: Loss: 0.5027 | Test Acc: 39.215% (602/1536)\n",
      "Evaluating: Batch 96/128: Loss: 0.5065 | Test Acc: 39.298% (609/1552)\n",
      "Evaluating: Batch 97/128: Loss: 0.5087 | Test Acc: 39.266% (615/1568)\n",
      "Evaluating: Batch 98/128: Loss: 0.5112 | Test Acc: 39.265% (621/1584)\n",
      "Evaluating: Batch 99/128: Loss: 0.5111 | Test Acc: 39.236% (627/1600)\n",
      "Evaluating: Batch 100/128: Loss: 0.5144 | Test Acc: 39.237% (634/1616)\n",
      "Evaluating: Batch 101/128: Loss: 0.5132 | Test Acc: 39.193% (639/1632)\n",
      "Evaluating: Batch 102/128: Loss: 0.5163 | Test Acc: 39.222% (646/1648)\n",
      "Evaluating: Batch 103/128: Loss: 0.5146 | Test Acc: 39.165% (651/1664)\n",
      "Evaluating: Batch 104/128: Loss: 0.5131 | Test Acc: 39.220% (658/1680)\n",
      "Evaluating: Batch 105/128: Loss: 0.5132 | Test Acc: 39.239% (665/1696)\n",
      "Evaluating: Batch 106/128: Loss: 0.5121 | Test Acc: 39.263% (672/1712)\n",
      "Evaluating: Batch 107/128: Loss: 0.5124 | Test Acc: 39.228% (677/1728)\n",
      "Evaluating: Batch 108/128: Loss: 0.5123 | Test Acc: 39.131% (682/1744)\n",
      "Evaluating: Batch 109/128: Loss: 0.5109 | Test Acc: 39.152% (689/1760)\n",
      "Evaluating: Batch 110/128: Loss: 0.5110 | Test Acc: 39.118% (694/1776)\n",
      "Evaluating: Batch 111/128: Loss: 0.5110 | Test Acc: 39.090% (700/1792)\n",
      "Evaluating: Batch 112/128: Loss: 0.5143 | Test Acc: 39.079% (706/1808)\n",
      "Evaluating: Batch 113/128: Loss: 0.5163 | Test Acc: 39.087% (712/1824)\n",
      "Evaluating: Batch 114/128: Loss: 0.5170 | Test Acc: 39.142% (720/1840)\n",
      "Evaluating: Batch 115/128: Loss: 0.5182 | Test Acc: 39.211% (727/1856)\n",
      "Evaluating: Batch 116/128: Loss: 0.5210 | Test Acc: 39.160% (733/1872)\n",
      "Evaluating: Batch 117/128: Loss: 0.5193 | Test Acc: 39.143% (739/1888)\n",
      "Evaluating: Batch 118/128: Loss: 0.5216 | Test Acc: 39.191% (746/1904)\n",
      "Evaluating: Batch 119/128: Loss: 0.5203 | Test Acc: 39.130% (751/1920)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 120/128: Loss: 0.5214 | Test Acc: 39.097% (756/1936)\n",
      "Evaluating: Batch 121/128: Loss: 0.5206 | Test Acc: 39.097% (763/1952)\n",
      "Evaluating: Batch 122/128: Loss: 0.5203 | Test Acc: 39.111% (769/1968)\n",
      "Evaluating: Batch 123/128: Loss: 0.5196 | Test Acc: 39.065% (775/1984)\n",
      "Evaluating: Batch 124/128: Loss: 0.5186 | Test Acc: 39.031% (780/2000)\n",
      "Evaluating: Batch 125/128: Loss: 0.5180 | Test Acc: 39.102% (788/2016)\n",
      "Evaluating: Batch 126/128: Loss: 0.5176 | Test Acc: 39.114% (794/2032)\n",
      "Evaluating: Batch 127/128: Loss: 0.5178 | Test Acc: 39.294% (800/2036)\n",
      "AVG test precision:0.3067263491239578\n",
      "AVG test recall:0.5466925864480765\n",
      "AVG test f1 score:0.38982092041021255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--- train ---\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1_score = []\n",
    "\n",
    "test_acc_all = 0\n",
    "\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(dev_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        test_total += len(target)\n",
    "        test_correct += torch.sum((pred > 0.5).float() == target) / torch.sum(target)\n",
    "        preds.extend(pred.argmax(1).cpu().numpy())\n",
    "        targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        test_acc_all += (test_correct / total)\n",
    "        \n",
    "        precision, recall, f1_score = calc_batch_metrics(target, pred)\n",
    "        \n",
    "        test_precision.append(precision)\n",
    "        test_recall.append(recall)\n",
    "        test_f1_score.append(f1_score)\n",
    "\n",
    "\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num, len(dev_loader), test_loss / (batch_num + 1), \n",
    "               100. * test_correct / test_total, test_correct, test_total))\n",
    "    \n",
    "    #cf_matrix = confusion_matrix(targets, preds)\n",
    "    #df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix) * 1000, \n",
    "    #                     index = [i for i in annotation_dict_train.keys()],\n",
    "    #                     columns=[i for i in annotation_dict_train.keys()])\n",
    "    #plt.figure(figsize=(12, 7))\n",
    "    #sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "    \n",
    "# print('AVG test acc:' + str(test_acc_all*100/len(dev_loader)) + '%')\n",
    "print('AVG test precision:' + str(np.mean(test_precision)))\n",
    "print('AVG test recall:' + str(np.mean(test_recall)))\n",
    "print('AVG test f1 score:' + str(np.mean(test_f1_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
