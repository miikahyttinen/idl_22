{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 5, 7, 2, 4, 6], [8, 10, 0]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse file nums to dicts\n",
    "def parseAnnotationNums(f):\n",
    "    lines = []\n",
    "    for line in f.readlines():\n",
    "        lines.append(line.replace('\\n', ''))\n",
    "    return lines\n",
    "        \n",
    "annotation_file_path = os.getcwd() + '/../data/annotations'\n",
    "annotations_files = os.listdir(annotation_file_path)\n",
    "annotation_dict_train = {}\n",
    "annotation_dict_dev = {}\n",
    "for file in annotations_files:\n",
    "    f = open(annotation_file_path  + '/' + file, \"r\")\n",
    "    annotation = file.split('.')\n",
    "    train_nums, dev_nums = train_test_split(parseAnnotationNums(f), test_size=0.2)\n",
    "    annotation_dict_train[annotation[0]] = train_nums\n",
    "    annotation_dict_dev[annotation[0]] = dev_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# RUN THIS CELL ONLY ONCE\n",
    "\n",
    "os.mkdir(os.getcwd() + '/../data_organized')\n",
    "os.mkdir(os.getcwd() + '/../data_organized/dev')\n",
    "os.mkdir(os.getcwd() + '/../data_organized/train')\n",
    "\n",
    "\n",
    "TRAIN_DESTINATION_PATH = os.getcwd() + '/../data_organized/train/'\n",
    "DEV_DESTINATION_PATH = os.getcwd() + '/../data_organized/dev/'\n",
    "\n",
    "img_file_path = os.getcwd() + '/../data/images/'\n",
    "\n",
    "for ann in annotation_dict_train.keys():\n",
    "    os.mkdir(TRAIN_DESTINATION_PATH + ann)\n",
    "    for num in annotation_dict_train[ann]:\n",
    "        img_file_name = 'im' + num + '.jpg'\n",
    "        shutil.copy(img_file_path + img_file_name, TRAIN_DESTINATION_PATH + ann)\n",
    "\n",
    "for ann in annotation_dict_dev.keys():\n",
    "    os.mkdir(DEV_DESTINATION_PATH + ann)\n",
    "    for num in annotation_dict_dev[ann]:\n",
    "        img_file_name = 'im' + num + '.jpg'\n",
    "        shutil.copy(img_file_path + img_file_name, DEV_DESTINATION_PATH + ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE_TRAIN = 128\n",
    "BATCH_SIZE_DEV = 16\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 14\n",
    "DATA_DIR = os.getcwd() + '/../data_organized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "train_transform = transforms.Compose([transforms.ColorJitter(),\n",
    "                                      transforms.RandomPerspective(distortion_scale=0.2),\n",
    "                                      transforms.ToTensor()])\n",
    "dev_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = datasets.ImageFolder(DATA_DIR + 'train', transform=train_transform)\n",
    "dev_set   = datasets.ImageFolder(DATA_DIR + 'dev',   transform=test_transform)\n",
    "# test_set  = datasets.ImageFolder(DATA_DIR % 'test',  transform=test_transform)\n",
    "\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_set, batch_size=BATCH_SIZE_DEV, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(32768, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n",
      "Training: Epoch 0 - Batch 0/32: Loss: 2.6182 | Dev Acc: 13.281% (17/128)\n",
      "Training: Epoch 0 - Batch 1/32: Loss: 7.0076 | Dev Acc: 18.750% (48/256)\n",
      "Training: Epoch 0 - Batch 2/32: Loss: 10.9143 | Dev Acc: 16.146% (62/384)\n",
      "Training: Epoch 0 - Batch 3/32: Loss: 10.8138 | Dev Acc: 15.234% (78/512)\n",
      "Training: Epoch 0 - Batch 4/32: Loss: 11.7013 | Dev Acc: 17.656% (113/640)\n",
      "Training: Epoch 0 - Batch 5/32: Loss: 12.1344 | Dev Acc: 17.318% (133/768)\n",
      "Training: Epoch 0 - Batch 6/32: Loss: 12.7417 | Dev Acc: 18.304% (164/896)\n",
      "Training: Epoch 0 - Batch 7/32: Loss: 12.8330 | Dev Acc: 17.578% (180/1024)\n",
      "Training: Epoch 0 - Batch 8/32: Loss: 13.3842 | Dev Acc: 16.580% (191/1152)\n",
      "Training: Epoch 0 - Batch 9/32: Loss: 13.3693 | Dev Acc: 15.703% (201/1280)\n",
      "Training: Epoch 0 - Batch 10/32: Loss: 13.1623 | Dev Acc: 15.696% (221/1408)\n",
      "Training: Epoch 0 - Batch 11/32: Loss: 12.7337 | Dev Acc: 15.039% (231/1536)\n",
      "Training: Epoch 0 - Batch 12/32: Loss: 12.1910 | Dev Acc: 15.505% (258/1664)\n",
      "Training: Epoch 0 - Batch 13/32: Loss: 11.9966 | Dev Acc: 15.792% (283/1792)\n",
      "Training: Epoch 0 - Batch 14/32: Loss: 11.8835 | Dev Acc: 16.042% (308/1920)\n",
      "Training: Epoch 0 - Batch 15/32: Loss: 11.6368 | Dev Acc: 16.211% (332/2048)\n",
      "Training: Epoch 0 - Batch 16/32: Loss: 11.3680 | Dev Acc: 16.222% (353/2176)\n",
      "Training: Epoch 0 - Batch 17/32: Loss: 11.0732 | Dev Acc: 16.710% (385/2304)\n",
      "Training: Epoch 0 - Batch 18/32: Loss: 10.9136 | Dev Acc: 17.188% (418/2432)\n",
      "Training: Epoch 0 - Batch 19/32: Loss: 10.6393 | Dev Acc: 17.344% (444/2560)\n",
      "Training: Epoch 0 - Batch 20/32: Loss: 10.4025 | Dev Acc: 17.634% (474/2688)\n",
      "Training: Epoch 0 - Batch 21/32: Loss: 10.1764 | Dev Acc: 17.685% (498/2816)\n",
      "Training: Epoch 0 - Batch 22/32: Loss: 9.9794 | Dev Acc: 17.493% (515/2944)\n",
      "Training: Epoch 0 - Batch 23/32: Loss: 9.7304 | Dev Acc: 17.415% (535/3072)\n",
      "Training: Epoch 0 - Batch 24/32: Loss: 9.5529 | Dev Acc: 17.594% (563/3200)\n",
      "Training: Epoch 0 - Batch 25/32: Loss: 9.4032 | Dev Acc: 17.728% (590/3328)\n",
      "Training: Epoch 0 - Batch 26/32: Loss: 9.1792 | Dev Acc: 18.084% (625/3456)\n",
      "Training: Epoch 0 - Batch 27/32: Loss: 9.0406 | Dev Acc: 17.913% (642/3584)\n",
      "Training: Epoch 0 - Batch 28/32: Loss: 8.8729 | Dev Acc: 17.780% (660/3712)\n",
      "Training: Epoch 0 - Batch 29/32: Loss: 8.7211 | Dev Acc: 17.682% (679/3840)\n",
      "Training: Epoch 0 - Batch 30/32: Loss: 8.5590 | Dev Acc: 17.792% (706/3968)\n",
      "Training: Epoch 0 - Batch 31/32: Loss: 8.4049 | Dev Acc: 18.004% (729/4049)\n",
      "Epoch train loss: 268.9558095932007\n",
      "Previous epoch train loss: 1000000\n",
      "Training: Epoch 1 - Batch 0/32: Loss: 4.1693 | Dev Acc: 16.406% (21/128)\n",
      "Training: Epoch 1 - Batch 1/32: Loss: 3.7613 | Dev Acc: 19.922% (51/256)\n",
      "Training: Epoch 1 - Batch 2/32: Loss: 3.4288 | Dev Acc: 23.438% (90/384)\n",
      "Training: Epoch 1 - Batch 3/32: Loss: 3.2899 | Dev Acc: 23.633% (121/512)\n",
      "Training: Epoch 1 - Batch 4/32: Loss: 3.1972 | Dev Acc: 24.062% (154/640)\n",
      "Training: Epoch 1 - Batch 5/32: Loss: 3.1278 | Dev Acc: 25.000% (192/768)\n",
      "Training: Epoch 1 - Batch 6/32: Loss: 3.1289 | Dev Acc: 24.442% (219/896)\n",
      "Training: Epoch 1 - Batch 7/32: Loss: 3.1007 | Dev Acc: 24.023% (246/1024)\n",
      "Training: Epoch 1 - Batch 8/32: Loss: 3.0577 | Dev Acc: 23.872% (275/1152)\n",
      "Training: Epoch 1 - Batch 9/32: Loss: 2.9991 | Dev Acc: 24.141% (309/1280)\n",
      "Training: Epoch 1 - Batch 10/32: Loss: 2.9283 | Dev Acc: 24.716% (348/1408)\n",
      "Training: Epoch 1 - Batch 11/32: Loss: 2.8623 | Dev Acc: 24.870% (382/1536)\n",
      "Training: Epoch 1 - Batch 12/32: Loss: 2.8389 | Dev Acc: 25.120% (418/1664)\n",
      "Training: Epoch 1 - Batch 13/32: Loss: 2.8449 | Dev Acc: 24.665% (442/1792)\n",
      "Training: Epoch 1 - Batch 14/32: Loss: 2.8529 | Dev Acc: 24.479% (470/1920)\n",
      "Training: Epoch 1 - Batch 15/32: Loss: 2.8129 | Dev Acc: 24.805% (508/2048)\n",
      "Training: Epoch 1 - Batch 16/32: Loss: 2.8157 | Dev Acc: 25.597% (557/2176)\n",
      "Training: Epoch 1 - Batch 17/32: Loss: 2.8015 | Dev Acc: 25.564% (589/2304)\n",
      "Training: Epoch 1 - Batch 18/32: Loss: 2.7694 | Dev Acc: 25.946% (631/2432)\n",
      "Training: Epoch 1 - Batch 19/32: Loss: 2.7583 | Dev Acc: 26.016% (666/2560)\n",
      "Training: Epoch 1 - Batch 20/32: Loss: 2.7311 | Dev Acc: 26.116% (702/2688)\n",
      "Training: Epoch 1 - Batch 21/32: Loss: 2.7298 | Dev Acc: 25.888% (729/2816)\n",
      "Training: Epoch 1 - Batch 22/32: Loss: 2.7100 | Dev Acc: 25.781% (759/2944)\n",
      "Training: Epoch 1 - Batch 23/32: Loss: 2.7007 | Dev Acc: 25.684% (789/3072)\n",
      "Training: Epoch 1 - Batch 24/32: Loss: 2.6932 | Dev Acc: 25.406% (813/3200)\n",
      "Training: Epoch 1 - Batch 25/32: Loss: 2.6777 | Dev Acc: 25.511% (849/3328)\n",
      "Training: Epoch 1 - Batch 26/32: Loss: 2.6637 | Dev Acc: 25.666% (887/3456)\n",
      "Training: Epoch 1 - Batch 27/32: Loss: 2.6586 | Dev Acc: 25.586% (917/3584)\n",
      "Training: Epoch 1 - Batch 28/32: Loss: 2.6556 | Dev Acc: 25.431% (944/3712)\n",
      "Training: Epoch 1 - Batch 29/32: Loss: 2.6412 | Dev Acc: 25.391% (975/3840)\n",
      "Training: Epoch 1 - Batch 30/32: Loss: 2.6216 | Dev Acc: 25.353% (1006/3968)\n",
      "Training: Epoch 1 - Batch 31/32: Loss: 2.6087 | Dev Acc: 25.512% (1033/4049)\n",
      "Epoch train loss: 83.47851920127869\n",
      "Previous epoch train loss: 268.9558095932007\n",
      "Training: Epoch 2 - Batch 0/32: Loss: 2.6388 | Dev Acc: 25.000% (32/128)\n",
      "Training: Epoch 2 - Batch 1/32: Loss: 2.6444 | Dev Acc: 22.656% (58/256)\n",
      "Training: Epoch 2 - Batch 2/32: Loss: 2.3882 | Dev Acc: 24.740% (95/384)\n",
      "Training: Epoch 2 - Batch 3/32: Loss: 2.2712 | Dev Acc: 26.172% (134/512)\n",
      "Training: Epoch 2 - Batch 4/32: Loss: 2.2877 | Dev Acc: 26.406% (169/640)\n",
      "Training: Epoch 2 - Batch 5/32: Loss: 2.2607 | Dev Acc: 26.302% (202/768)\n",
      "Training: Epoch 2 - Batch 6/32: Loss: 2.2335 | Dev Acc: 27.567% (247/896)\n",
      "Training: Epoch 2 - Batch 7/32: Loss: 2.2126 | Dev Acc: 28.223% (289/1024)\n",
      "Training: Epoch 2 - Batch 8/32: Loss: 2.2208 | Dev Acc: 28.212% (325/1152)\n",
      "Training: Epoch 2 - Batch 9/32: Loss: 2.1983 | Dev Acc: 28.359% (363/1280)\n",
      "Training: Epoch 2 - Batch 10/32: Loss: 2.2096 | Dev Acc: 28.196% (397/1408)\n",
      "Training: Epoch 2 - Batch 11/32: Loss: 2.2195 | Dev Acc: 28.060% (431/1536)\n",
      "Training: Epoch 2 - Batch 12/32: Loss: 2.2092 | Dev Acc: 29.147% (485/1664)\n",
      "Training: Epoch 2 - Batch 13/32: Loss: 2.1981 | Dev Acc: 29.353% (526/1792)\n",
      "Training: Epoch 2 - Batch 14/32: Loss: 2.1969 | Dev Acc: 28.958% (556/1920)\n",
      "Training: Epoch 2 - Batch 15/32: Loss: 2.1911 | Dev Acc: 28.467% (583/2048)\n",
      "Training: Epoch 2 - Batch 16/32: Loss: 2.1784 | Dev Acc: 28.493% (620/2176)\n",
      "Training: Epoch 2 - Batch 17/32: Loss: 2.1802 | Dev Acc: 28.950% (667/2304)\n",
      "Training: Epoch 2 - Batch 18/32: Loss: 2.1751 | Dev Acc: 29.276% (712/2432)\n",
      "Training: Epoch 2 - Batch 19/32: Loss: 2.1750 | Dev Acc: 28.828% (738/2560)\n",
      "Training: Epoch 2 - Batch 20/32: Loss: 2.1836 | Dev Acc: 28.274% (760/2688)\n",
      "Training: Epoch 2 - Batch 21/32: Loss: 2.1709 | Dev Acc: 28.303% (797/2816)\n",
      "Training: Epoch 2 - Batch 22/32: Loss: 2.2003 | Dev Acc: 28.227% (831/2944)\n",
      "Training: Epoch 2 - Batch 23/32: Loss: 2.2026 | Dev Acc: 28.418% (873/3072)\n",
      "Training: Epoch 2 - Batch 24/32: Loss: 2.2130 | Dev Acc: 28.219% (903/3200)\n",
      "Training: Epoch 2 - Batch 25/32: Loss: 2.2104 | Dev Acc: 28.095% (935/3328)\n",
      "Training: Epoch 2 - Batch 26/32: Loss: 2.2042 | Dev Acc: 28.328% (979/3456)\n",
      "Training: Epoch 2 - Batch 27/32: Loss: 2.1970 | Dev Acc: 28.516% (1022/3584)\n",
      "Training: Epoch 2 - Batch 28/32: Loss: 2.1920 | Dev Acc: 28.745% (1067/3712)\n",
      "Training: Epoch 2 - Batch 29/32: Loss: 2.1934 | Dev Acc: 28.594% (1098/3840)\n",
      "Training: Epoch 2 - Batch 30/32: Loss: 2.1925 | Dev Acc: 28.654% (1137/3968)\n",
      "Training: Epoch 2 - Batch 31/32: Loss: 2.1941 | Dev Acc: 28.526% (1155/4049)\n",
      "Epoch train loss: 70.21059167385101\n",
      "Previous epoch train loss: 83.47851920127869\n",
      "Training: Epoch 3 - Batch 0/32: Loss: 1.9756 | Dev Acc: 28.125% (36/128)\n",
      "Training: Epoch 3 - Batch 1/32: Loss: 1.9156 | Dev Acc: 30.469% (78/256)\n",
      "Training: Epoch 3 - Batch 2/32: Loss: 1.8984 | Dev Acc: 32.552% (125/384)\n",
      "Training: Epoch 3 - Batch 3/32: Loss: 1.9829 | Dev Acc: 31.250% (160/512)\n",
      "Training: Epoch 3 - Batch 4/32: Loss: 2.0010 | Dev Acc: 30.781% (197/640)\n",
      "Training: Epoch 3 - Batch 5/32: Loss: 1.9936 | Dev Acc: 31.120% (239/768)\n",
      "Training: Epoch 3 - Batch 6/32: Loss: 2.0313 | Dev Acc: 31.138% (279/896)\n",
      "Training: Epoch 3 - Batch 7/32: Loss: 2.0384 | Dev Acc: 31.250% (320/1024)\n",
      "Training: Epoch 3 - Batch 8/32: Loss: 2.0400 | Dev Acc: 31.684% (365/1152)\n",
      "Training: Epoch 3 - Batch 9/32: Loss: 2.0132 | Dev Acc: 32.656% (418/1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 3 - Batch 10/32: Loss: 2.0564 | Dev Acc: 32.031% (451/1408)\n",
      "Training: Epoch 3 - Batch 11/32: Loss: 2.0733 | Dev Acc: 30.990% (476/1536)\n",
      "Training: Epoch 3 - Batch 12/32: Loss: 2.0539 | Dev Acc: 31.430% (523/1664)\n",
      "Training: Epoch 3 - Batch 13/32: Loss: 2.0621 | Dev Acc: 31.696% (568/1792)\n",
      "Training: Epoch 3 - Batch 14/32: Loss: 2.0666 | Dev Acc: 31.979% (614/1920)\n",
      "Training: Epoch 3 - Batch 15/32: Loss: 2.0571 | Dev Acc: 32.178% (659/2048)\n",
      "Training: Epoch 3 - Batch 16/32: Loss: 2.0570 | Dev Acc: 32.031% (697/2176)\n",
      "Training: Epoch 3 - Batch 17/32: Loss: 2.0527 | Dev Acc: 32.075% (739/2304)\n",
      "Training: Epoch 3 - Batch 18/32: Loss: 2.0392 | Dev Acc: 32.401% (788/2432)\n",
      "Training: Epoch 3 - Batch 19/32: Loss: 2.0446 | Dev Acc: 32.461% (831/2560)\n",
      "Training: Epoch 3 - Batch 20/32: Loss: 2.0477 | Dev Acc: 32.292% (868/2688)\n",
      "Training: Epoch 3 - Batch 21/32: Loss: 2.0450 | Dev Acc: 32.173% (906/2816)\n",
      "Training: Epoch 3 - Batch 22/32: Loss: 2.0488 | Dev Acc: 31.963% (941/2944)\n",
      "Training: Epoch 3 - Batch 23/32: Loss: 2.0398 | Dev Acc: 32.357% (994/3072)\n",
      "Training: Epoch 3 - Batch 24/32: Loss: 2.0432 | Dev Acc: 32.406% (1037/3200)\n",
      "Training: Epoch 3 - Batch 25/32: Loss: 2.0461 | Dev Acc: 32.572% (1084/3328)\n",
      "Training: Epoch 3 - Batch 26/32: Loss: 2.0465 | Dev Acc: 32.407% (1120/3456)\n",
      "Training: Epoch 3 - Batch 27/32: Loss: 2.0491 | Dev Acc: 32.227% (1155/3584)\n",
      "Training: Epoch 3 - Batch 28/32: Loss: 2.0500 | Dev Acc: 32.220% (1196/3712)\n",
      "Training: Epoch 3 - Batch 29/32: Loss: 2.0617 | Dev Acc: 32.005% (1229/3840)\n",
      "Training: Epoch 3 - Batch 30/32: Loss: 2.0652 | Dev Acc: 31.779% (1261/3968)\n",
      "Training: Epoch 3 - Batch 31/32: Loss: 2.0731 | Dev Acc: 31.489% (1275/4049)\n",
      "Epoch train loss: 66.34031844139099\n",
      "Previous epoch train loss: 70.21059167385101\n",
      "Training: Epoch 4 - Batch 0/32: Loss: 2.0377 | Dev Acc: 28.125% (36/128)\n",
      "Training: Epoch 4 - Batch 1/32: Loss: 2.0834 | Dev Acc: 31.250% (80/256)\n",
      "Training: Epoch 4 - Batch 2/32: Loss: 2.1011 | Dev Acc: 32.552% (125/384)\n",
      "Training: Epoch 4 - Batch 3/32: Loss: 2.0302 | Dev Acc: 32.812% (168/512)\n",
      "Training: Epoch 4 - Batch 4/32: Loss: 2.0012 | Dev Acc: 33.438% (214/640)\n",
      "Training: Epoch 4 - Batch 5/32: Loss: 2.0116 | Dev Acc: 31.380% (241/768)\n",
      "Training: Epoch 4 - Batch 6/32: Loss: 1.9943 | Dev Acc: 31.808% (285/896)\n",
      "Training: Epoch 4 - Batch 7/32: Loss: 1.9372 | Dev Acc: 33.691% (345/1024)\n",
      "Training: Epoch 4 - Batch 8/32: Loss: 1.9497 | Dev Acc: 33.681% (388/1152)\n",
      "Training: Epoch 4 - Batch 9/32: Loss: 1.9484 | Dev Acc: 32.969% (422/1280)\n",
      "Training: Epoch 4 - Batch 10/32: Loss: 1.9520 | Dev Acc: 32.599% (459/1408)\n",
      "Training: Epoch 4 - Batch 11/32: Loss: 1.9461 | Dev Acc: 33.203% (510/1536)\n",
      "Training: Epoch 4 - Batch 12/32: Loss: 1.9788 | Dev Acc: 33.353% (555/1664)\n",
      "Training: Epoch 4 - Batch 13/32: Loss: 1.9619 | Dev Acc: 33.705% (604/1792)\n",
      "Training: Epoch 4 - Batch 14/32: Loss: 1.9618 | Dev Acc: 33.333% (640/1920)\n",
      "Training: Epoch 4 - Batch 15/32: Loss: 1.9818 | Dev Acc: 33.252% (681/2048)\n",
      "Training: Epoch 4 - Batch 16/32: Loss: 1.9798 | Dev Acc: 32.812% (714/2176)\n",
      "Training: Epoch 4 - Batch 17/32: Loss: 1.9787 | Dev Acc: 32.812% (756/2304)\n",
      "Training: Epoch 4 - Batch 18/32: Loss: 2.0066 | Dev Acc: 32.113% (781/2432)\n",
      "Training: Epoch 4 - Batch 19/32: Loss: 2.0007 | Dev Acc: 32.305% (827/2560)\n",
      "Training: Epoch 4 - Batch 20/32: Loss: 2.0118 | Dev Acc: 32.515% (874/2688)\n",
      "Training: Epoch 4 - Batch 21/32: Loss: 2.0107 | Dev Acc: 32.528% (916/2816)\n",
      "Training: Epoch 4 - Batch 22/32: Loss: 2.0114 | Dev Acc: 32.167% (947/2944)\n",
      "Training: Epoch 4 - Batch 23/32: Loss: 2.0087 | Dev Acc: 31.901% (980/3072)\n",
      "Training: Epoch 4 - Batch 24/32: Loss: 2.0082 | Dev Acc: 32.000% (1024/3200)\n",
      "Training: Epoch 4 - Batch 25/32: Loss: 2.0193 | Dev Acc: 31.881% (1061/3328)\n",
      "Training: Epoch 4 - Batch 26/32: Loss: 2.0248 | Dev Acc: 31.800% (1099/3456)\n",
      "Training: Epoch 4 - Batch 27/32: Loss: 2.0288 | Dev Acc: 31.613% (1133/3584)\n",
      "Training: Epoch 4 - Batch 28/32: Loss: 2.0301 | Dev Acc: 31.519% (1170/3712)\n",
      "Training: Epoch 4 - Batch 29/32: Loss: 2.0399 | Dev Acc: 31.615% (1214/3840)\n",
      "Training: Epoch 4 - Batch 30/32: Loss: 2.0455 | Dev Acc: 31.754% (1260/3968)\n",
      "Training: Epoch 4 - Batch 31/32: Loss: 2.0395 | Dev Acc: 31.736% (1285/4049)\n",
      "Epoch train loss: 65.26475524902344\n",
      "Previous epoch train loss: 66.34031844139099\n",
      "Training: Epoch 5 - Batch 0/32: Loss: 1.6594 | Dev Acc: 36.719% (47/128)\n",
      "Training: Epoch 5 - Batch 1/32: Loss: 1.7316 | Dev Acc: 34.766% (89/256)\n",
      "Training: Epoch 5 - Batch 2/32: Loss: 1.7261 | Dev Acc: 35.938% (138/384)\n",
      "Training: Epoch 5 - Batch 3/32: Loss: 1.7391 | Dev Acc: 36.523% (187/512)\n",
      "Training: Epoch 5 - Batch 4/32: Loss: 1.7524 | Dev Acc: 37.344% (239/640)\n",
      "Training: Epoch 5 - Batch 5/32: Loss: 1.7541 | Dev Acc: 36.458% (280/768)\n",
      "Training: Epoch 5 - Batch 6/32: Loss: 1.7145 | Dev Acc: 38.058% (341/896)\n",
      "Training: Epoch 5 - Batch 7/32: Loss: 1.7292 | Dev Acc: 38.574% (395/1024)\n",
      "Training: Epoch 5 - Batch 8/32: Loss: 1.7385 | Dev Acc: 39.062% (450/1152)\n",
      "Training: Epoch 5 - Batch 9/32: Loss: 1.7606 | Dev Acc: 38.203% (489/1280)\n",
      "Training: Epoch 5 - Batch 10/32: Loss: 1.7828 | Dev Acc: 37.571% (529/1408)\n",
      "Training: Epoch 5 - Batch 11/32: Loss: 1.7780 | Dev Acc: 37.695% (579/1536)\n",
      "Training: Epoch 5 - Batch 12/32: Loss: 1.7934 | Dev Acc: 37.620% (626/1664)\n",
      "Training: Epoch 5 - Batch 13/32: Loss: 1.8098 | Dev Acc: 37.612% (674/1792)\n",
      "Training: Epoch 5 - Batch 14/32: Loss: 1.8110 | Dev Acc: 37.292% (716/1920)\n",
      "Training: Epoch 5 - Batch 15/32: Loss: 1.8136 | Dev Acc: 37.158% (761/2048)\n",
      "Training: Epoch 5 - Batch 16/32: Loss: 1.8079 | Dev Acc: 37.224% (810/2176)\n",
      "Training: Epoch 5 - Batch 17/32: Loss: 1.8064 | Dev Acc: 37.066% (854/2304)\n",
      "Training: Epoch 5 - Batch 18/32: Loss: 1.8142 | Dev Acc: 37.171% (904/2432)\n",
      "Training: Epoch 5 - Batch 19/32: Loss: 1.8220 | Dev Acc: 36.758% (941/2560)\n",
      "Training: Epoch 5 - Batch 20/32: Loss: 1.8160 | Dev Acc: 36.905% (992/2688)\n",
      "Training: Epoch 5 - Batch 21/32: Loss: 1.8314 | Dev Acc: 36.470% (1027/2816)\n",
      "Training: Epoch 5 - Batch 22/32: Loss: 1.8276 | Dev Acc: 36.685% (1080/2944)\n",
      "Training: Epoch 5 - Batch 23/32: Loss: 1.8339 | Dev Acc: 36.491% (1121/3072)\n",
      "Training: Epoch 5 - Batch 24/32: Loss: 1.8299 | Dev Acc: 36.438% (1166/3200)\n",
      "Training: Epoch 5 - Batch 25/32: Loss: 1.8250 | Dev Acc: 36.629% (1219/3328)\n",
      "Training: Epoch 5 - Batch 26/32: Loss: 1.8289 | Dev Acc: 36.574% (1264/3456)\n",
      "Training: Epoch 5 - Batch 27/32: Loss: 1.8346 | Dev Acc: 36.356% (1303/3584)\n",
      "Training: Epoch 5 - Batch 28/32: Loss: 1.8355 | Dev Acc: 36.126% (1341/3712)\n",
      "Training: Epoch 5 - Batch 29/32: Loss: 1.8391 | Dev Acc: 35.859% (1377/3840)\n",
      "Training: Epoch 5 - Batch 30/32: Loss: 1.8368 | Dev Acc: 35.938% (1426/3968)\n",
      "Training: Epoch 5 - Batch 31/32: Loss: 1.8514 | Dev Acc: 35.787% (1449/4049)\n",
      "Epoch train loss: 59.24378192424774\n",
      "Previous epoch train loss: 65.26475524902344\n",
      "Training: Epoch 6 - Batch 0/32: Loss: 1.8520 | Dev Acc: 35.156% (45/128)\n",
      "Training: Epoch 6 - Batch 1/32: Loss: 1.8123 | Dev Acc: 35.938% (92/256)\n",
      "Training: Epoch 6 - Batch 2/32: Loss: 1.8395 | Dev Acc: 37.240% (143/384)\n",
      "Training: Epoch 6 - Batch 3/32: Loss: 1.8196 | Dev Acc: 36.523% (187/512)\n",
      "Training: Epoch 6 - Batch 4/32: Loss: 1.7399 | Dev Acc: 37.031% (237/640)\n",
      "Training: Epoch 6 - Batch 5/32: Loss: 1.7174 | Dev Acc: 37.240% (286/768)\n",
      "Training: Epoch 6 - Batch 6/32: Loss: 1.6893 | Dev Acc: 38.170% (342/896)\n",
      "Training: Epoch 6 - Batch 7/32: Loss: 1.6639 | Dev Acc: 39.062% (400/1024)\n",
      "Training: Epoch 6 - Batch 8/32: Loss: 1.7095 | Dev Acc: 37.847% (436/1152)\n",
      "Training: Epoch 6 - Batch 9/32: Loss: 1.7012 | Dev Acc: 38.281% (490/1280)\n",
      "Training: Epoch 6 - Batch 10/32: Loss: 1.7076 | Dev Acc: 38.494% (542/1408)\n",
      "Training: Epoch 6 - Batch 11/32: Loss: 1.7185 | Dev Acc: 38.867% (597/1536)\n",
      "Training: Epoch 6 - Batch 12/32: Loss: 1.7118 | Dev Acc: 38.942% (648/1664)\n",
      "Training: Epoch 6 - Batch 13/32: Loss: 1.7082 | Dev Acc: 38.616% (692/1792)\n",
      "Training: Epoch 6 - Batch 14/32: Loss: 1.7225 | Dev Acc: 37.917% (728/1920)\n",
      "Training: Epoch 6 - Batch 15/32: Loss: 1.7174 | Dev Acc: 38.086% (780/2048)\n",
      "Training: Epoch 6 - Batch 16/32: Loss: 1.7080 | Dev Acc: 38.465% (837/2176)\n",
      "Training: Epoch 6 - Batch 17/32: Loss: 1.7138 | Dev Acc: 38.368% (884/2304)\n",
      "Training: Epoch 6 - Batch 18/32: Loss: 1.7171 | Dev Acc: 37.993% (924/2432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 6 - Batch 19/32: Loss: 1.7128 | Dev Acc: 38.164% (977/2560)\n",
      "Training: Epoch 6 - Batch 20/32: Loss: 1.7185 | Dev Acc: 37.946% (1020/2688)\n",
      "Training: Epoch 6 - Batch 21/32: Loss: 1.7285 | Dev Acc: 37.749% (1063/2816)\n",
      "Training: Epoch 6 - Batch 22/32: Loss: 1.7316 | Dev Acc: 37.670% (1109/2944)\n",
      "Training: Epoch 6 - Batch 23/32: Loss: 1.7339 | Dev Acc: 37.793% (1161/3072)\n",
      "Training: Epoch 6 - Batch 24/32: Loss: 1.7400 | Dev Acc: 37.594% (1203/3200)\n",
      "Training: Epoch 6 - Batch 25/32: Loss: 1.7556 | Dev Acc: 37.560% (1250/3328)\n",
      "Training: Epoch 6 - Batch 26/32: Loss: 1.7590 | Dev Acc: 37.529% (1297/3456)\n",
      "Training: Epoch 6 - Batch 27/32: Loss: 1.7680 | Dev Acc: 37.305% (1337/3584)\n",
      "Training: Epoch 6 - Batch 28/32: Loss: 1.7688 | Dev Acc: 37.258% (1383/3712)\n",
      "Training: Epoch 6 - Batch 29/32: Loss: 1.7773 | Dev Acc: 36.927% (1418/3840)\n",
      "Training: Epoch 6 - Batch 30/32: Loss: 1.7811 | Dev Acc: 36.920% (1465/3968)\n",
      "Training: Epoch 6 - Batch 31/32: Loss: 1.7869 | Dev Acc: 36.824% (1491/4049)\n",
      "Epoch train loss: 57.179877161979675\n",
      "Previous epoch train loss: 59.24378192424774\n",
      "Training: Epoch 7 - Batch 0/32: Loss: 1.3392 | Dev Acc: 50.781% (65/128)\n",
      "Training: Epoch 7 - Batch 1/32: Loss: 1.6363 | Dev Acc: 44.922% (115/256)\n",
      "Training: Epoch 7 - Batch 2/32: Loss: 1.7113 | Dev Acc: 43.490% (167/384)\n",
      "Training: Epoch 7 - Batch 3/32: Loss: 1.6790 | Dev Acc: 43.164% (221/512)\n",
      "Training: Epoch 7 - Batch 4/32: Loss: 1.6465 | Dev Acc: 43.906% (281/640)\n",
      "Training: Epoch 7 - Batch 5/32: Loss: 1.6743 | Dev Acc: 43.620% (335/768)\n",
      "Training: Epoch 7 - Batch 6/32: Loss: 1.6525 | Dev Acc: 44.420% (398/896)\n",
      "Training: Epoch 7 - Batch 7/32: Loss: 1.6598 | Dev Acc: 44.824% (459/1024)\n",
      "Training: Epoch 7 - Batch 8/32: Loss: 1.6539 | Dev Acc: 44.184% (509/1152)\n",
      "Training: Epoch 7 - Batch 9/32: Loss: 1.6511 | Dev Acc: 43.750% (560/1280)\n",
      "Training: Epoch 7 - Batch 10/32: Loss: 1.6399 | Dev Acc: 44.034% (620/1408)\n",
      "Training: Epoch 7 - Batch 11/32: Loss: 1.6577 | Dev Acc: 43.750% (672/1536)\n",
      "Training: Epoch 7 - Batch 12/32: Loss: 1.6419 | Dev Acc: 44.050% (733/1664)\n",
      "Training: Epoch 7 - Batch 13/32: Loss: 1.6462 | Dev Acc: 43.750% (784/1792)\n",
      "Training: Epoch 7 - Batch 14/32: Loss: 1.6451 | Dev Acc: 43.646% (838/1920)\n",
      "Training: Epoch 7 - Batch 15/32: Loss: 1.6312 | Dev Acc: 43.799% (897/2048)\n",
      "Training: Epoch 7 - Batch 16/32: Loss: 1.6313 | Dev Acc: 43.704% (951/2176)\n",
      "Training: Epoch 7 - Batch 17/32: Loss: 1.6288 | Dev Acc: 43.576% (1004/2304)\n",
      "Training: Epoch 7 - Batch 18/32: Loss: 1.6416 | Dev Acc: 42.887% (1043/2432)\n",
      "Training: Epoch 7 - Batch 19/32: Loss: 1.6352 | Dev Acc: 43.125% (1104/2560)\n",
      "Training: Epoch 7 - Batch 20/32: Loss: 1.6335 | Dev Acc: 42.820% (1151/2688)\n",
      "Training: Epoch 7 - Batch 21/32: Loss: 1.6417 | Dev Acc: 42.045% (1184/2816)\n",
      "Training: Epoch 7 - Batch 22/32: Loss: 1.6369 | Dev Acc: 42.052% (1238/2944)\n",
      "Training: Epoch 7 - Batch 23/32: Loss: 1.6533 | Dev Acc: 41.829% (1285/3072)\n",
      "Training: Epoch 7 - Batch 24/32: Loss: 1.6512 | Dev Acc: 42.062% (1346/3200)\n",
      "Training: Epoch 7 - Batch 25/32: Loss: 1.6556 | Dev Acc: 42.067% (1400/3328)\n",
      "Training: Epoch 7 - Batch 26/32: Loss: 1.6562 | Dev Acc: 41.840% (1446/3456)\n",
      "Training: Epoch 7 - Batch 27/32: Loss: 1.6539 | Dev Acc: 41.657% (1493/3584)\n",
      "Training: Epoch 7 - Batch 28/32: Loss: 1.6527 | Dev Acc: 41.730% (1549/3712)\n",
      "Training: Epoch 7 - Batch 29/32: Loss: 1.6526 | Dev Acc: 41.589% (1597/3840)\n",
      "Training: Epoch 7 - Batch 30/32: Loss: 1.6498 | Dev Acc: 41.633% (1652/3968)\n",
      "Training: Epoch 7 - Batch 31/32: Loss: 1.6548 | Dev Acc: 41.615% (1685/4049)\n",
      "Epoch train loss: 52.952088952064514\n",
      "Previous epoch train loss: 57.179877161979675\n",
      "Training: Epoch 8 - Batch 0/32: Loss: 1.4477 | Dev Acc: 46.094% (59/128)\n",
      "Training: Epoch 8 - Batch 1/32: Loss: 1.5625 | Dev Acc: 44.531% (114/256)\n",
      "Training: Epoch 8 - Batch 2/32: Loss: 1.4619 | Dev Acc: 49.219% (189/384)\n",
      "Training: Epoch 8 - Batch 3/32: Loss: 1.4398 | Dev Acc: 49.414% (253/512)\n",
      "Training: Epoch 8 - Batch 4/32: Loss: 1.4016 | Dev Acc: 49.062% (314/640)\n",
      "Training: Epoch 8 - Batch 5/32: Loss: 1.4637 | Dev Acc: 46.875% (360/768)\n",
      "Training: Epoch 8 - Batch 6/32: Loss: 1.4636 | Dev Acc: 46.875% (420/896)\n",
      "Training: Epoch 8 - Batch 7/32: Loss: 1.4826 | Dev Acc: 47.070% (482/1024)\n",
      "Training: Epoch 8 - Batch 8/32: Loss: 1.4631 | Dev Acc: 47.743% (550/1152)\n",
      "Training: Epoch 8 - Batch 9/32: Loss: 1.4873 | Dev Acc: 46.875% (600/1280)\n",
      "Training: Epoch 8 - Batch 10/32: Loss: 1.4983 | Dev Acc: 46.449% (654/1408)\n",
      "Training: Epoch 8 - Batch 11/32: Loss: 1.5029 | Dev Acc: 46.484% (714/1536)\n",
      "Training: Epoch 8 - Batch 12/32: Loss: 1.5161 | Dev Acc: 46.274% (770/1664)\n",
      "Training: Epoch 8 - Batch 13/32: Loss: 1.5097 | Dev Acc: 46.484% (833/1792)\n",
      "Training: Epoch 8 - Batch 14/32: Loss: 1.5325 | Dev Acc: 45.312% (870/1920)\n",
      "Training: Epoch 8 - Batch 15/32: Loss: 1.5292 | Dev Acc: 45.508% (932/2048)\n",
      "Training: Epoch 8 - Batch 16/32: Loss: 1.5350 | Dev Acc: 45.037% (980/2176)\n",
      "Training: Epoch 8 - Batch 17/32: Loss: 1.5286 | Dev Acc: 45.095% (1039/2304)\n",
      "Training: Epoch 8 - Batch 18/32: Loss: 1.5241 | Dev Acc: 45.230% (1100/2432)\n",
      "Training: Epoch 8 - Batch 19/32: Loss: 1.5343 | Dev Acc: 45.039% (1153/2560)\n",
      "Training: Epoch 8 - Batch 20/32: Loss: 1.5409 | Dev Acc: 45.052% (1211/2688)\n",
      "Training: Epoch 8 - Batch 21/32: Loss: 1.5478 | Dev Acc: 45.135% (1271/2816)\n",
      "Training: Epoch 8 - Batch 22/32: Loss: 1.5479 | Dev Acc: 45.109% (1328/2944)\n",
      "Training: Epoch 8 - Batch 23/32: Loss: 1.5549 | Dev Acc: 44.759% (1375/3072)\n",
      "Training: Epoch 8 - Batch 24/32: Loss: 1.5551 | Dev Acc: 44.594% (1427/3200)\n",
      "Training: Epoch 8 - Batch 25/32: Loss: 1.5590 | Dev Acc: 44.591% (1484/3328)\n",
      "Training: Epoch 8 - Batch 26/32: Loss: 1.5648 | Dev Acc: 44.531% (1539/3456)\n",
      "Training: Epoch 8 - Batch 27/32: Loss: 1.5607 | Dev Acc: 44.587% (1598/3584)\n",
      "Training: Epoch 8 - Batch 28/32: Loss: 1.5584 | Dev Acc: 44.531% (1653/3712)\n",
      "Training: Epoch 8 - Batch 29/32: Loss: 1.5561 | Dev Acc: 44.427% (1706/3840)\n",
      "Training: Epoch 8 - Batch 30/32: Loss: 1.5570 | Dev Acc: 44.355% (1760/3968)\n",
      "Training: Epoch 8 - Batch 31/32: Loss: 1.5504 | Dev Acc: 44.505% (1802/4049)\n",
      "Epoch train loss: 49.61323440074921\n",
      "Previous epoch train loss: 52.952088952064514\n",
      "Training: Epoch 9 - Batch 0/32: Loss: 1.3193 | Dev Acc: 46.875% (60/128)\n",
      "Training: Epoch 9 - Batch 1/32: Loss: 1.2239 | Dev Acc: 50.000% (128/256)\n",
      "Training: Epoch 9 - Batch 2/32: Loss: 1.2569 | Dev Acc: 49.740% (191/384)\n",
      "Training: Epoch 9 - Batch 3/32: Loss: 1.2219 | Dev Acc: 50.977% (261/512)\n",
      "Training: Epoch 9 - Batch 4/32: Loss: 1.2390 | Dev Acc: 50.312% (322/640)\n",
      "Training: Epoch 9 - Batch 5/32: Loss: 1.2482 | Dev Acc: 50.781% (390/768)\n",
      "Training: Epoch 9 - Batch 6/32: Loss: 1.2466 | Dev Acc: 51.786% (464/896)\n",
      "Training: Epoch 9 - Batch 7/32: Loss: 1.2577 | Dev Acc: 51.074% (523/1024)\n",
      "Training: Epoch 9 - Batch 8/32: Loss: 1.2476 | Dev Acc: 51.910% (598/1152)\n",
      "Training: Epoch 9 - Batch 9/32: Loss: 1.2359 | Dev Acc: 51.797% (663/1280)\n",
      "Training: Epoch 9 - Batch 10/32: Loss: 1.2473 | Dev Acc: 52.202% (735/1408)\n",
      "Training: Epoch 9 - Batch 11/32: Loss: 1.2406 | Dev Acc: 52.734% (810/1536)\n",
      "Training: Epoch 9 - Batch 12/32: Loss: 1.2366 | Dev Acc: 53.065% (883/1664)\n",
      "Training: Epoch 9 - Batch 13/32: Loss: 1.2388 | Dev Acc: 53.125% (952/1792)\n",
      "Training: Epoch 9 - Batch 14/32: Loss: 1.2453 | Dev Acc: 52.865% (1015/1920)\n",
      "Training: Epoch 9 - Batch 15/32: Loss: 1.2515 | Dev Acc: 52.344% (1072/2048)\n",
      "Training: Epoch 9 - Batch 16/32: Loss: 1.2435 | Dev Acc: 52.895% (1151/2176)\n",
      "Training: Epoch 9 - Batch 17/32: Loss: 1.2367 | Dev Acc: 53.038% (1222/2304)\n",
      "Training: Epoch 9 - Batch 18/32: Loss: 1.2418 | Dev Acc: 52.961% (1288/2432)\n",
      "Training: Epoch 9 - Batch 19/32: Loss: 1.2440 | Dev Acc: 53.008% (1357/2560)\n",
      "Training: Epoch 9 - Batch 20/32: Loss: 1.2526 | Dev Acc: 52.493% (1411/2688)\n",
      "Training: Epoch 9 - Batch 21/32: Loss: 1.2569 | Dev Acc: 52.166% (1469/2816)\n",
      "Training: Epoch 9 - Batch 22/32: Loss: 1.2611 | Dev Acc: 52.242% (1538/2944)\n",
      "Training: Epoch 9 - Batch 23/32: Loss: 1.2686 | Dev Acc: 51.953% (1596/3072)\n",
      "Training: Epoch 9 - Batch 24/32: Loss: 1.2755 | Dev Acc: 51.938% (1662/3200)\n",
      "Training: Epoch 9 - Batch 25/32: Loss: 1.2760 | Dev Acc: 51.893% (1727/3328)\n",
      "Training: Epoch 9 - Batch 26/32: Loss: 1.2782 | Dev Acc: 51.852% (1792/3456)\n",
      "Training: Epoch 9 - Batch 27/32: Loss: 1.2761 | Dev Acc: 51.925% (1861/3584)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 9 - Batch 28/32: Loss: 1.2829 | Dev Acc: 51.751% (1921/3712)\n",
      "Training: Epoch 9 - Batch 29/32: Loss: 1.2878 | Dev Acc: 51.615% (1982/3840)\n",
      "Training: Epoch 9 - Batch 30/32: Loss: 1.2858 | Dev Acc: 51.689% (2051/3968)\n",
      "Training: Epoch 9 - Batch 31/32: Loss: 1.2847 | Dev Acc: 51.741% (2095/4049)\n",
      "Epoch train loss: 41.11091351509094\n",
      "Previous epoch train loss: 49.61323440074921\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAGbCAYAAACh0BXiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABJsUlEQVR4nO3dd5xddZ3/8df3tumTSTIz6b0nlBBCaAIBQhXFgpSfq+zqilh+omsv6yquu+5PRX/+cEVcXVERZVGkiEACoQRCSSC99zLJlEym13vv9/fHObdMy/RyTt7PxyOPO3PvnbnnzIV5z+f7/Xy/x1hrERER8arAcB+AiIhIfyjIRETE0xRkIiLiaQoyERHxNAWZiIh4Wmi4D6AzhYWFdvr06cN9GCIiMkKsX7++wlpb1NljIzLIpk+fzrp164b7MEREZIQwxhzs6jENLYqIiKcpyERExNMUZCIi4mkKMhER8TQFmYiIeJqCTEREPE1BJiIinqYgExERT1OQiYiIpynIRETE0xRkIiLiaQoyERHxNAWZiIh4moJMREQ8zfdBVt3YOtyHICIig8jXQXbkZANLvrOSDYerhvtQRERkkPg6yCrrW4jFLWU1TcN9KCIiMkh8HWRx2/ZWRET8x+dB5iSYtUoyERG/8nWQJQJMFZmIiH/5OsgSAWZRkomI+JW/gyyuikxExO/8HWSJikxzZCIivuXrIEvNkSnIRET8ytdBlqrIhvc4RERk8Pg8yDRHJiLid6dJkCnJRET8ytdBZtXsISLie74OMg0tioj4n8+DzLlVQSYi4l8+DzLNkYmI+J2vg8xq02AREd/zdZDpMi4iIv7n8yDruiJ7ZU8FX35k01AfkoiIDDCfB1nb23Rr9lTw8PrDQ3tAIiIy4HwdZKfaazEet1ir+TMREa/zdZClhhY7PhZ1y7SYJtBERDwt1N0TjDG/Am4Ayqy1Z7j3/RGY5z6lAKiy1i7u5GsPALVADIhaa5cOyFH3UDzu3HZ2Yc1EgMWs7f6HICIiI1ZPfof/GrgX+E3iDmvtLYmPjTE/BKpP8fWXW2sr+nqA/XGqnT0Sj6kiExHxtm6DzFr7kjFmemePGWMMcDNwxQAf14CwyWaPjmGloUUREX/o7xzZJUCptXZ3F49b4FljzHpjzB2n+kbGmDuMMeuMMevKy8v7eViOU82Rxd0ASww/ioiIN/U3yG4DHjrF4xdba5cA1wGfMsZc2tUTrbX3W2uXWmuXFhUV9fOwHMn2+06qrvQ5MhER8a4+B5kxJgS8D/hjV8+x1pa4t2XAo8Cyvr5eXyQrsk4ei2loUUTEF/pTka0Adlhrj3T2oDEmxxiTl/gYuBrY0o/X67VTrSOLaUNhERFf6DbIjDEPAWuBecaYI8aYj7oP3Uq7YUVjzERjzFPup+OANcaYjcAbwF+ttU8P3KF371Q7eyQqsagqMhERT+tJ1+JtXdz/953cVwJc7368Dzi7n8fXL6faazGWbPZQkImIeJnPd/ZwbjsbPdQcmYiIP/g6yE6516JV16KIiB/4OshOtbNHVEOLIiK+4PMgc25PNUemZg8REW/zeZD1YGhRQSYi4mm+DjJ7imaPaEzryERE/MDXQZbcT1G734uI+Ja/g+wUu98n15GpIhMR8TSfB1n3C6Jj2v1eRMTTfB1k9lSbBttE16KSTETEy3wdZKceWnQfU46JiHiaz4Os62aPmJtg2tlDRMTbfB1kiYjSpsEiIv7l6yBLNXt0fEybBouI+EO3l3HxMtvJHFlzNIa16c0eCjIRES/zdZB1tiD6C/+ziXjcJps8tI5MRMTb/B1knVRkJVWNxK1Ntt1raFFExNt8HmQd58haookAa/scERHxJl8Hme1kZ4+WaBxjtNeiiIhf+DrIUkOLqftaYnGCAUM0pqFFERE/OC3a7+PtKrJoLJ4MNwWZiIi3nRYVWfo0WHM0DgRS68g0RyYi4mm+DrLUpsHpFVmMYEA7e4iI+MXpMbSYtjFwSyxONGaTlZiGFkVEvM3nQZa4bTtH1hqLpw0tDseRiYjIQPF5kLXd2SPR5NGSdjXNmK7jIiLiab4OMpts9nA+SARYczQ9yIb8sEREZAD5Osji7a4QndjVI71RUTt7iIh4m8+DLHHrVmTRjuWXmj1ERLzN50HWdo6sWUEmIuI7vg6y9nsttnYyIaYgExHxNl8HWaIhMTEN1tJZkGmOTETE0/wdZO32Wuxsjkw7e4iIeJvPgyxxq2YPERG/8nWQ2XbNHp0GmYYWRUQ8zddBllwjluha7GSOTEOLIiLe5vMgS9x2PbQYVZCJiHiary/jkt7scfN9a8nOCHb5HBER8SZfB5lNVmTwxoHKTp+jZg8REW/z+dBi2wXRndGmwSIi3tZtkBljfmWMKTPGbEm771vGmKPGmA3uv+u7+NprjTE7jTF7jDFfGcgD74n2W1Sd6jkiIuJNPanIfg1c28n9P7LWLnb/PdX+QWNMEPgpcB2wELjNGLOwPwfbW4kA62xrqgQ1e4iIeFu3QWatfQnofILp1JYBe6y1+6y1LcAfgBv78H36LDGkeKqwUvu9iIi39WeO7NPGmE3u0OPoTh6fBBxO+/yIe1+njDF3GGPWGWPWlZeX9+OwUhIZdaqGDjV7iIh4W1+D7GfALGAxcAz4YSfPMZ3c12VqWGvvt9YutdYuLSoq6uNhtRU/xa73CdrZQ0TE2/oUZNbaUmttzFobB36BM4zY3hFgStrnk4GSvrxeXyWKrWhMQ4siIn7VpyAzxkxI+/S9wJZOnvYmMMcYM8MYEwFuBR7vy+v1VWqOTBWZiIhfdbsg2hjzELAcKDTGHAH+BVhujFmMM1R4APi4+9yJwH9Za6+31kaNMZ8GngGCwK+stVsH4yS6Eu+m2SMcNJojExHxuG6DzFp7Wyd3/7KL55YA16d9/hTQoTV/qCQKsfShxYxQgGZ3z8VwMKAgExHxuNNiZ4/0Zo+cjFR2K8hERLzP10GWmP5KH1rMjqQ2Do6EAtrZQ0TE43wdZImQSq+6ciKpiiyiikxExPNOiyBLl34pl4yQgkxExOt8HWSdjRqmDy2GgwG134uIeJyvg6yziiwzFCTg7jkSDhldxkVExON8HmQd74uEAoSCzmlHggHt7CEi4nE+D7KOITUmJ0LYLck0tCgi4n2+DrL2GfVv7z2TL107P1WRhVSRiYh4na+DrH1FNqEgk1FZYcJBpyLLCAV0YU0REY87rYIsaJwACwWc09bOHiIi3ufrIGs/tBh058ZCwdQcmXb2EBHxNl8HWftiK2BSAQbOHJkqMhERb/N1kNn2Q4uJiiygikxExC98HWQd5siSQ4sBjHGuR6ZmDxERb/N5kLX9PJisxAxBYwgYXVhTRMTrfB5kXXUtGoLuP60jExHxNl8HWfvpL7frnlAgkAwy7ewhIuJtvg6yrufIUkOLcW0aLCLiaadXkJlUs0cwaAipIhMR8TyfB1nbzwOJZo+AW5EFnGaP9m36IiLiHb4Osg7ryExqaDHghhl0frkXERHxBl8HWVft96FgwO1cdO5XC76IiHf5PMjaBlRiaDEjFEhWZZ09T0REvCM03AcwWKy1WOusGUvs3pEYSrzj0pm86+yJ7DpeC6giExHxMt9WZIkiKzGcCKl1ZPPH53P5vOLkY9qmSkTEu3wbZInhwvQgS1Rkyc8TQ4sKMhERz/JxkDm3bYIs0HmQaS2ZiIh3+TjInHAKtRla7CLIVJGJiHiWb4MsNUeWOsX2Q4uJC2y2RLVPlYiIV/k2yFJzZKn72g8tZoTcIIspyEREvMr3QRZKr8jaBVlEFZmIiOf5OMic21N1LSaGFltVkYmIeJZvg8z2oNkjElJFJiLidb4NsvYVWfthRVCQiYj4gY+DrO2C6PbDipAKsmYNLYqIeNZpE2SBTs400ezRqopMRMSzfBtkiXVkoR5UZGq/FxHxLt8GWceKrJMgU/u9iIjn+TjInFs1e4iI+Fu3QWaM+ZUxpswYsyXtvu8bY3YYYzYZYx41xhR08bUHjDGbjTEbjDHrBvC4u5XY0b4nzR4aWhQR8a6eVGS/Bq5td99K4Axr7VnALuCrp/j6y621i621S/t2iH2TmiNzTrGzoUXttSgi4n3dBpm19iWgst19z1pro+6nrwGTB+HY+qUn7ffaa1FExPsGYo7sI8DfunjMAs8aY9YbY+441TcxxtxhjFlnjFlXXl7e74PqEGRq9hAR8aV+BZkx5utAFHiwi6dcbK1dAlwHfMoYc2lX38tae7+1dqm1dmlRUVF/Dgvo2OzR2TqyQMAQChgFmYiIh/U5yIwxtwM3AB+0tvNLLFtrS9zbMuBRYFlfX6+32u+12NnQIjjzZNo0WETEu/oUZMaYa4EvA++21jZ08ZwcY0xe4mPgamBLZ88dDD1pvwenc1EVmYiId/Wk/f4hYC0wzxhzxBjzUeBeIA9Y6bbW3+c+d6Ix5in3S8cBa4wxG4E3gL9aa58elLPoRLx9RXaqIFNFJiLiWaHunmCtva2Tu3/ZxXNLgOvdj/cBZ/fr6PohEWSJtvtAF0OLkWCAZlVkIiKe5dudPTrstXiKiqw11ukUn4iIeIBvgyzVfh9wb7uuyFqisSE7LhERGVg+DjLnNtTd0KKaPUREPM3HQdb9gmhQs4eIiNf5NshsD7aogsTQooJMRMSrfBtkHYYWuzjTcChAi5o9REQ8y79B1v4yLqds9lBFJiLiVf4NsvZ7LXYxtJgRUteiiIiX+TbIOsyRqdlDRMSXfBtkyYrMdLdpsKE1qjkyERGv8nGQuRVZMNHsoYpMRMSPfBtkiRqru4osEgyq2UNExMN8G2S9WhCtIBMR8SzfBlmHC2t22X5vaInF6eLaoCIiMsL5NsjibpEVDHazaXDIeVw74IuIeJN/gywxtGi63zQYUMOHiIhH+TjInNvU0GLnz4u4D2ieTETEm3wbZD1fEB0EFGQiIl7l2yDr6RZVYXedWauGFkVEPMm3QWbpefs9QLMqMhERT/JtkPVm02DQ0KKIiFf5NsgSc2SJAOuuIlPXooiIN/k4yJzb7hZEh9wrbsbiCjIRES/ybZC136Kqq6HFRNBpQbSIiDf5NsgSFVmgm3VkIfeBqIJMRMSTfBtk8fZ7LXZVkSXa7zW0KCLiSb4NsmRFZk59PbJE0MVUkYmIeJJ/g8xdR5aouLqsyNxmj6gqMhERT/JtkHVYR9ZFRZba2UMVmYiIF/k4yNruft9l+30w0X6vIBMR8SLfBpltV5GFupkj016LIiLe5OMg6+E6MndoMaqKTETEk3wbZO3nyLrb2SOqikxExJN8G2SJiqwwN4P/fcVsrphf3OnzEkOLqshERLwpNNwHMFiSFZkxfP7qeV0+Lzm0qK5FERFP8m1FluhapPMRxaSw27WonT1ERLzJt0GW0MXUWFJyaDGtInt8Ywmv7TsxmIclIiIDxLdBFm93PbKuBDuZI/v+Mzt44NUDg3ZsIiIycHwcZM5tNzmGMYZQwLTpWqxqaNW6MhERj/BtkLXfNPhUQkGTrMhicUttU5QWNX+IiHhCt0FmjPmVMabMGLMl7b4xxpiVxpjd7u3oLr72WmPMTmPMHmPMVwbywLuTGFrsQY4RDgSSc2Q1ja0AtERjg3ZsIiIycHpSkf0auLbdfV8BnrPWzgGecz9vwxgTBH4KXAcsBG4zxizs19H2QmIdmemubREIBk1y9/tqN8i0ibCIiDd0G2TW2peAynZ33wg84H78APCeTr50GbDHWrvPWtsC/MH9uiGRGlrs/rmhQCAZXFXJINMcmYiIF/R1jmyctfYYgHvb2bYZk4DDaZ8fce/rlDHmDmPMOmPMuvLy8j4eVkqq2aP7JAsHU80e1cmhRQWZiIgXDGazR2cJ0uV4nbX2fmvtUmvt0qKion6/eOLCmj2pyIIBk7yMS1VDC6CKTETEK/oaZKXGmAkA7m1ZJ885AkxJ+3wyUNLH1+u13lVkAVrjbZs9NEcmIuINfQ2yx4Hb3Y9vBx7r5DlvAnOMMTOMMRHgVvfrhoS1tkcdi0CbdWRVDZojExHxkp603z8ErAXmGWOOGGM+CnwPuMoYsxu4yv0cY8xEY8xTANbaKPBp4BlgO/CwtXbr4JxGR9b2bA0ZOFeJTqwj0xyZiIi3dLv7vbX2ti4eurKT55YA16d9/hTwVJ+Prh/i1vZofgzaVWSJIFNFJiLiCb7d2SNue7aGDNru7FGt9nsREU/xbZBZej5HFg4EksGlBdEiIt7i3yDrxRxZevt9tdvsEYvb5H0iIjJy+TbI4vFedC0GTbICS1RkoOFFEREv8G2QWXpekYWDgeRei7VNrckAVJCJiIx8vg2yeK/XkTkVWWvckhkKAmrBFxHxAt8GmbWd75HVmfbXI8uKOEGmhg8RkZHPx0FmCfRwIVkoECAai2Ot0+CRFU4EmSoyEZGRzrdBFu/Vzh5Os0eiSTFRkWlRtIjIyOfjILM9HloMBwLE4jbZ8KGKTETEO3wbZJae7XwPqStEJ9aNJYMsqjkyEZGRzr9B1ou9FsMBZ2gx0fCREXZ+LBpaFBEZ+XwbZPE4vVgQ7TR7xGJtKzK134uIjHy+DTKL7XmzR8Bpv49ZN8gimiMTEfEK3wZZvA/ryDrMkSnIRERGPB8Hme1xs0fI7VpMBJcqMhER7/BtkGEh0MOzCwedwGtqbdt+36KdPURERjzfBpmzjqynl3FxfgzN0RgAmcn2e1VkIiIjnW+DzNn9vmfPTVRkzVEtiBYR8RrfBlmvtqgKJIYW3YpMW1SJiHiGj4Os522LoWBiaLHdHJmGFkVERjzfBhl9qMia3YosNbSoZg8RkZHOt0EW78UWVYmKLNG1mK32exERz/B1kPW0azHV7BFzPw8QMAoyEREv8G2QWdvzvRaDgbZdi8GAIRwMqNlDRMQDfBtkvetaTAwtOhVZKGiIBAO6jIuIiAf4NsistT2uyNrv7BEMGMKhAC2x2GAdnoiIDBD/Bhm9qMiCbXf2CAUM4aBRRSYi4gG+DbJedS0m2++diixgnDkyNXuIiIx8Pg4yetztkdzZI9p2jkzNHiIiI59vg8z2Yx1ZKGCIhFSRiYh4gY+DrA87eyTb7wPu0KIzRxaLWw6daBicAxURkX7xbZA5C6J7JhRsu2lwstnDrcie3XqcK374ApX1LYNxqCIi0g++DbLeVGSRYNt1ZIkF0YkKraKumWjcUtvUOjgHKyIifebbIIv3ah1ZxyCLhFJBlrjVnJmIyMjj2yDrzRZV4ZDzY2hMC7L8zHCyAksEWYvWlYmIjDj+DTJsr4cWG1tSc2T5WWFqGqNA6rpk0bgqMhGRkca3QRbvRUUWadd+HwwY8rNC1DS2Yq3V0KKIyAjm2yBz1pH18DIuIed5DS1OBRYKBBiVFaYlFqepNZ6syHShTRGRkce3QeZUZD29HlnHObJRWWEAappak3swqiITERl5+hxkxph5xpgNaf9qjDGfbfec5caY6rTnfLPfR9xDtjfryAIdd7/Pz3SCrLqxNTVHpopMRGTECfX1C621O4HFAMaYIHAUeLSTp75srb2hr6/TV87u9z17rjFOu30isAKGZEVW3dia3HNRey+KiIw8AzW0eCWw11p7cIC+X7/FezFHBqmGj1DAYEza0GJja3JXfFVkIiIjz0AF2a3AQ108dqExZqMx5m/GmEVdfQNjzB3GmHXGmHXl5eX9PqB4vOddi5C6uGbQLePyO6nINEcmIjLy9DvIjDER4N3A/3Ty8FvANGvt2cD/A/7S1fex1t5vrV1qrV1aVFTU38PCuYpLz5MsnFaRAW0qssSQo4YWRURGnoGoyK4D3rLWlrZ/wFpbY62tcz9+CggbYwoH4DW71ZvLuEAqyJIVWaYzfVjdGE12LWpoUURk5BmIILuNLoYVjTHjjVsWGWOWua93YgBes1vO7vc9T7KMUNsgCwUD5ESCbboWNbQoIjLy9LlrEcAYkw1cBXw87b47Aay19wE3AZ8wxkSBRuBWa+2QlDXWQqAXMZ2qyFJfNCor7K4jU5CJiIxU/Qoya20DMLbdffelfXwvcG9/XqOvnN3vezFH5u7uEUobj8zPCreryDS0KCIy0vh2Zw9r6cXAYsc5MkgFWXNyQbQqMhGRkca/QUbPL6wJaV2LwdTXjMoKO12Lar8XERmxfBtk8V52LbZv9oBUkDW7ezC2xjW0KCIy0vRrjmwk6/UcWWJoMe1r8jJD1DZHk5VYa1QVmYjISOPbIOvNFaKh484eAHkZIeqao8m5Ng0tioiMPL4Osv7OkeVmhrDWmW8DDS2KiIxEvp4j603XYqSTdWR57qVcEjS0KCIy8vg2yHpbkUVCbfdaBMjNaFuwRlWRiYiMOL4Nsri1fdvZo12zRzptGiwiMvL4OMigN0uiO1sQ3T7ItCBaRGTk8W2QQS93v09sUZXe7JHRbo5MW1SJiIw4vg2yeC/b7yM9qMjUfi8iMvL4Nsic65H1otkj2Emzh4JMRGTE822QxXu7jqyTLapyI6kgCxgNLYqIjEQ+DrLehU5nzR6BgEm24OdEQmr2EBEZgXwbZPR2HVlyi6q2P5LEPFlORogWVWQiIiOOb4Ost7vfhzuZI4PUouicjKAqMhGREcjHQdbbTYM7Di1CquEjNyOkZg8RkRHIt0Fm6WXXYidbVEFqv8WcjJCaPURERiDfBplTkfVvZw9wLuUCkB1RRSYiMhL5Nsistb1bEB3qeD0ySDV75GYEtWmwiMgI5OMgo0/NHh3myDJSXYu6jIuIyMjj2yCL93Jnj666FmcW5TIuP8Ntv1eQiYiMND4Ost7sfZ9q9mi/juzW86bw8peuIBw0GloUERmBfBlk1t3VozfNHp3ttQjO7h6RUIBwMEAsbokrzERERhSfBplz25ehxfZzZO0fb41reFFEZCTxZZDFkxVZz78mHOy8a7H941pLJiIysvgyyBJRMxBdi+0f1zZVIiIjiy+DLN6HObKMLnb2SAi5QabORRGRkcWXQZaYIxuIvRYTErvj3/PsLirrW6hpak3u9LGnrI7ntpf2/YBFRKTPfB1kvWn2KMgO8+Vr53PtGeM7fTzktuX/4c3DPLzuMGd961m+/MgmAFbc8yIffWBd/w5aRET6JNT9U7wnObTYi68xxvCJ5bO6fDxxBWmA3649CMDKbarCRESGmy8rskSQ9aYi6044bcjxaFUjAOdMG51csyYiIsPDl0GWiJYBzDE6i6vcjCDHa5oG7kVERKTX/BlkbmNhb7oWu1PXHAXg5qWTk/c1tcbZV16f/DymXT9ERIacL+fILImhxYH7nu9ZPInmaJzbzpvChbPG8uNVu2lqjbG3vC75nNZYnGAgOHAvKiIi3fJlRRbvQ9didyKhAB+6YBqhYID3njOZKaOzaWqNtanIGlpirD94csBeU0REuufTIOv9FlW9lRkO0ByNc6iyIXnfk5tKeP/PXqXEbQYREZHB58sgSy2IHrwkywgHaWqNJefOAMprmwGorG8ZtNcVEZG2fBpkAz9H1l5mKEhTa5yGllSQ1TY5H9enhZuIiAyufgWZMeaAMWazMWaDMabD1hbG8RNjzB5jzCZjzJL+vF5PJebITK+WRPeOM7QYo6EllrwvEWD1LQoyEZGhMhBdi5dbayu6eOw6YI7773zgZ+7toBqMrsX2MsNORRYOxsjPDFHTFE0OM9Y1x7r5ahERGSiDPbR4I/Ab63gNKDDGTBjk1xyUrsX2MkIBmlpj1DdHKciOAKm1ZhpaFBEZOv0NMgs8a4xZb4y5o5PHJwGH0z4/4t7XgTHmDmPMOmPMuvLy8n4dVDw1tjhoMsNBonFLfUuMguwwoDkyEZHh0N8gu9hauwRnCPFTxphL2z3eWZR0uv2FtfZ+a+1Sa+3SoqKifh6WYzArssyw86OLxS2jspwgSw0tKshERIZKv4LMWlvi3pYBjwLL2j3lCDAl7fPJQEl/XrMn4kPRtRhO7eCR7wZZvYYWRUSGXJ+DzBiTY4zJS3wMXA1safe0x4EPu92LFwDV1tpjfT7aHor34cKavZUZSgVZQaIia1Kzh4jIUOtP1+I44FF30XEI+L219mljzJ0A1tr7gKeA64E9QAPwD/073J6xg3AZl/Yywqm/ARJzZHUtqshERIZan4PMWrsPOLuT++9L+9gCn+rra/RVfAh29kgfWkzMkSV2FFGQiYgMHV/v7DGII4tkpF0xOhFkCWr2EBEZOv4MMvd2cLsW0yuySJvH0nf7EBGRweXLIBvqrsXEHFmChhZFRIaOP4MseYXowXuNzLCGFkVERgJfBllir8VBbfYIqSITERkJ/Blkg79DVdsF0ZntgqwlltomS0REBpWvg2ywNw1OyAoHiQTb/igPVTYkuydFRGTw+DLIks0eg3h2iYosKxwkEDCEg05oJm6X/+AFHtsw6LtxiYic9nwdZIN5Yc1ERZYdcQIt7H4+JifVir+rtHbQXl9ERBy+DLLEgN5gdi0GAoZIKEB2hhNkiaHF7Ehqs5SoO09W3dCqTkYRkUHizyAbgr0WATJDAbLDTnCF3SCbPz4v+XhFbTMAd/x2Hd94dPOgHouIyOnKl0E2FLvfgzNPluUOLUbcocXFUwrY8Z1rOXvyKMrrnCDbW17H4ZONg3swIiKnKV8G2VB0LYITZMk5MrfJIxIKkBkOUpibQUVdCy3ROBV1LVQ1tAzqsYiInK58GWTJZo9BrsiyI0FyM5yhxURFluEulHaCrJmy2iYAqhs1RyYiMhj6cz2yEWsouhYBvv3uReS5i6ETc2SJQCvMi1BZ38Kx6kSQtWCtHdTdRkRETke+DDKSQ4uD+zLnzxyb/DgRZIm2/KLcDGJxy47jTgt+a8zS0BIjJ8OfP3IRkeHi06FF5zYw2EmWJhFgqYosA4CtR6uTz6lqbB2y4xEROV34NMgG/8Ka7bWvyApznSDbUpIWZGr4EBEZcL4MstSC6KGLsvSuRUgLsqM1yedUqyITERlwvgyyobiwZnupiszpWixyhxYB8tx5seoGBZmIyEDzZZBZO/jXI2sv1X7v3I7KCnPVwnEAyW2sNEcmIjLwfBpkzu1QVmSRdnNkAD++ZTGXzCnkk8tnA1ClikxEZMD5MsjiQ7SzR7r268gAcjJC/Paj5/PhC6cRCQaoalSzh4jIQPNpkA39BS3bz5GlM8YwKjtMjYYWRUQGnC+DbKj2WkwXCXWsyNIVZIU1tCgiMgh8GmRDs9diuojbfp/RVZBlK8hERAaDL4NsOObICvMyyMsIdRlko7IinNSCaBGRAefLjf8sQ7+O7JbzprBiwThCwc6DrCgvwobDJ4fugEREThO+rsiGcmgxIxRkYkFWl48X5WVyor6FaCw+dAclInIa8GWQDceC6O4U5WVgLZyo1/CiiMhA8mmQObdDOUfWnWJ3y6ry2ubkfXXNUW68dw1vHdKQo4hIX/kyyIZj9/vuJPZeTFwxGpxLvGw8Us0TG0uG67BERDzPl0HmlYpsT3kdAK/vqxyWYxIR8QNfBll8GNaRdSdZkdWkBVmZE2Tbj9d0eYmX8tpmfv7iXppaY4N/kCIiHuTP9vth6FrsTkYoyKisMOV1bYMsEgzQEouz/mAlV8wf1+ZrjlY1cvH3ngdgRmEOVy8aP6THLCLiBb6syFLryEZQkuEMLz61+Th3/eFtjlc3sbesjisXFBMJBViz+0SH5z+/vTT58cmGFp7Zepzj1U0dnjfS1DS1UqnuTBEZIr4MsuHY2aMnxuZGqKhr5rENJbzr3jWUVDexaGI+F80ay/M7SpPLBhL2ltcnq8ry2mY+8bv13Pfi3mE48t75l8e28vHfrhvuwxCR04RPg2zkzZEBHKhoAOCuK+eQFXZ2yZ9VlMuV84s5cKKBveX1bZ6/t7yOMyaOIhIMsKesjriFbSU1Q37cvXW0qpH97rmKiAw2XwbZSJwjA/jq9fO5ZtE47rpyDo996mLuvnERVy4YxxULnLmx59KGEgH2ltUxuziX0TlhdruNIduO1RCPpyq3uuYoL+4qH7qT6IGaxlZO1DdrFxMRGRI+DbKROUd24+JJ/PxDSwkEDKNzInz4wulEQgEmFWQxpziXV/em5snqm6OUVDcxqyiH0dkR9rqt+nXNUQ6fTFU7D795mNt/9QalNT2fO9tfUc/TW45R1xwduJNLU9sU1S4mIjJk+hxkxpgpxpjVxpjtxpitxpi7OnnOcmNMtTFmg/vvm/073J5J7rU4FC82QJZOH8Nbh04mq639Fc4w46yiXAqywzS1pqqb9OHFIycbATh4omdDeQcq6rn8By9w5+/e4k/rjwzU4bdR2+QsJUhfMyciMlj6U5FFgc9baxcAFwCfMsYs7OR5L1trF7v/7u7H6/XY+FGZXDRrLOEuLqkyEp07bTS1TdHkEGKiAptVnMuYnEjyecbA1pIaymqb2Fdex7FqJ8gOVXYMsnjc8tPVe7jn2Z3J+3aV1iY/HoygiccttW6ll76LiYjIYOnzOjJr7THgmPtxrTFmOzAJ2DZAx9Zn1ywazzUeW3O1dNpoANYdrGTe+Dz2Vzgdi1PHZFOQ7QRZZjjA5NHZ7Dhey91PbGNrSQ2jssIAHK5sYP3BSh5Zf4SPXTKTmUW5/Otft/OrV/YDcNeKuQQDJhl4AcOgXB+tviWanKMc6KCMxS0nG1oozM0Y0O87mB7bcJRZRbmcMWnUcB+KiG8NSMlijJkOnAO83snDFxpjNhpj/maMWXSK73GHMWadMWZdefnIal4YCtPGZlOYG2H9QWcD4UMnGpiQn0lmOMjobCesxuZkMKsoh30VdWw/VsOBE/UcOOEMQa7cVsoH7lvLQ28c5vevHyIet/z57dTQ4VF3CPLIyUbyMkJML8wZlCtW1zSl5t1e3l3BD57Z2WFZQV/d9+Jelv7rKk+spUv4+qNb+NWa/cN9GCK+1u8gM8bkAn8CPmutbd8b/hYwzVp7NvD/gL909X2stfdba5daa5cWFRX197A8xxjDkqmjU0FW2cCUMdkAjHYrssLcCLOKcjl0ooGDJxqwlmQYbTtWQ9zCvHF5rN13gh3Ha6lqaOWD508FYHdZbZvvOzo7QlXjwFdkifkxgCc3HePe1XuS83j99do+pxnmzQMDtzdlczTG718/NCgdlnXNUeqao212cxGRgdevIDPGhHFC7EFr7Z/bP26trbHW1rkfPwWEjTGF/XlNP1s6fTQHTzRQXtvMwcoGpo1tH2QZzCzKJRq3RNNa8EPupbAXTMjn+jMnsO1YDU9vOQbAhy+cDpCceztU2eAMV2aFOVnvhM4b+yt5ZU9Fl8fV030eH9twlDf3dwyZHcdrO3l2701yL1w6kEG2alsZX3t0M6t39n0U4M9vHeFzf9zQ4f5EJ2lFnbo3RQZTf7oWDfBLYLu19p4unjPefR7GmGXu63Xci0kAOHfaGABe2VNBeW0zUxMVWY47tJgbYVZRToevO2uyM/9yyZxCLpw1Fmvhv185wLSx2cwbn8e4/Ax2ldZireVwZQNTxmRRkB2hyp0j+/e/bedLj2xq8z0PVNTTHI2x/Purmf/PT/P/ntt9ymM/Wd/CPz28kR+u3AVAbkZq+nXHsYFZxF3jVnuJymwg7DzuHNuLu8q4+edreXl37wNt1fZSntxUkhxCTfxcE0F2qrnCldtKaWhJDcfe+/xuPvabdQM2HDtQ1uyuYOPhquE+DJFO9aciuxj4EHBFWnv99caYO40xd7rPuQnYYozZCPwEuNWOtP9DR5AzJuUTCQV49O2jAMmhxYJ2FVlC4tIwF8wcC8DFswtZPKWAguwwtc1RrjtjAgBzivPYU1ZHeW0zzdE4U8dkMzo7zEl3WPJwZQNHqxo57DaCvLSrnOU/eIFH3zrKAbet//mdZac89lXbS4nFbXKoMxG4+ZmhDhVZZX0LrbE4h040sL1dyCWuCADwwKsH2gRLYv/GXaV1nBig4brEsf3hjcO8sb+Sv2053uvvUVLVRGvMUt3Yypaj1Sz5zko2HalKBlllfTM1Ta3JgEvYW17Hx36zjh+vSv2R8JcNJazcVspz20/98x5q33xsC99/Zmf3TxQZBv3pWlxDN0u1rLX3Avf29TVONxmhIGdNGpXcqWPaWCcMxrhBNjY3g1FZYQpzM4gEDQsm5PPcjjJuv2g6s4tzuWR2IYGA4bl/usz5Ordtf3ZxLn9883ByC6wpY7KpaYrS2BqjqqElOfT1+v5KpozJ5gV3mO2hNw4BsGJBMS/uKqepNUYkGKA5GicrEqSirpm7n9jGN25YwDNb2+5K8t5zJjG7OI/apla2H0+FVTQW54afvExxfiZlNU2EggFe+tLlAGw+Us277l3D/R86lyvmF/NvT213zmuOM2d6sr6V3IwQdc1RdpbWctEAdC8mliMkhmq3HK3u9fcoqXLmACvqmp21gBbePlRFozskG7fwqQff4mRDC0/+70uSX5d4rYfeOMRnrpxDLGaTQf7j53axYuE4RgJrLceqm5Jbv4mMNN5ZaHWa+F9ucwaQHFqcMiabj186k2sWOb/Yls0YzbIZYzhz8igKczMozsvgfUsmE3DnysbmZjA2NwN3VJcLZo6hsTXGT1fvAZy5tETb/ua0X9yvu0N2r+515ss2HqkmHDTcdO5kWmOWjYer+MZjW7j0+6s5Vt3IM1uP8/jGEr731A5e3l1OOJj6u+bWZVP54c1nM39CPgcq6mlscX6prz94kpLqJjYcrqKkuolDlQ1UuNXVtmPOsfz5raPsr6inORpna0kNB9zF4SfqW5LDqHvK6rjpZ69yz7M7aW3XqFHT1Mr/rDucnNsrrWkiFrfE4paWaOq5DS1RDlY2sGy6M6SbEwmy41gt1Q2tvHmgMhlQp9ISjSebOcprW5IV3q7S2jbdlWv3nmDL0Zo2VdnWkhqMcXZC+dIjG1npblF28eyxbDla06ZxZjD8cs1+NvRguLCm0fmj52hVI7H40IXZmt0V7CkbmPlV8TcF2Qjz3nMmMXecM3yYaLsPBgxfvX4Bk0c7wXbvbUv40S2L+cTyWTz92UuSgdWVd8wpIhw0rNlTwZKpBYzLz0w2kGw64oTHpIIsVm4v5c0DlW2GAmcV5XL+DGfo8q+bj/Hwm4cpr23mroc28Ibb2PHnt49igTsunQlAJBQg090U+dxpo4lbeHyjM1y6clspkWCAu29clHx+Yu5ln1sxPr+zjNfSmkYS808nG1o4Y9IoAgZWbS9j3cGT/OT5Pdz3QtsrAvz+9UN88ZFNXPPjl9h8pJpL/mM1f3zzMLf94jXm/fPf+McH1nHv87tZ+M1nsBZuv2g6X7t+Pl975wJaYnGW/2A1H7hvLZ948K1u36/SmqbkurmKumZ2pgVZ+oLwRMWX6EoFZ4eWRRPzuevKOazaVsYXH9lIwMBN504G6LCJ9EBqjcX57l+38bvXDnb73OPuEGlrzA7ZIvd43PLJB9fzH09rOFO6pyAbYYwx/PUzl7DuGyu6DKhAwGCMISMU7NHi4NyMUHIe7foznXmzREhuOlIFwA9vPpugMXzgvrWAUxUAzB2Xx+icCGdMyuc3aw8St5Y7L5vFGwcqeWrzseRc2GeumM3Fs52G1PzMcPK1L51TyDlTC/jhs7soq2nimW3HuWj2WD584XQ+u2IOwYBJBtne8noywwFaonH+c/UeIsEA500fzY9W7eZHq3YTi1uK8zKYMCqL19x9KcfmRFi1o4w/v3WEP77pDIXuLk11aP7Dr9+gJRbnlT0VrDtQyaKJ+azaXsoPnt2VPMYzJuVzx6WzeId7/CcbWjl32mi2l9QQjcXZdKSKW+9fm2w2+eWa/ckqMb1qK6ttZlcyyOo4Xt2U7LRMePPASeqbo1hr2VpSzaIJo/jcVXP57384D+P+vM+aXOD8PNLmC/viVNPRx6ubiFtnvWJ3jqft49ndUopPPrieb/xlc88Psgv7KuqoaYp2uNrDUFaE4FTcR3tQmcvwUpCNQOFgYMB3r7j2jPGEg4Zrz3B2PEk0kGw+Uk12JMj5M8bwp09cxKcvn80Xrp7LB8+fBsC88XkA3P+hpdyydAqfvnw2n7tqDoW5EVpjlg9fOJ3nP38Zn7p8NlPcijE/MzX1aozhG+9cyIn6Fi783vMcOdnIbcuc4dPsSIi54/LY4FaF+yrquGxuEWdMyudYdRPzxufxiw8v5fwZY/iJ2zU5JifClDFZtMTiRIIBbls2lc1HqvjGX7bw5T9t5oWdZewpr+Pi2WO5/swJyfm/VdtLiVv49OVz+MEHzuZjl8xg/TdW8Mc7LkjORU4dk83EUZncsnQKHzx/Ki2xOPsr6nnwtUO8tq+SlVtLqaxv4TtPbmP5D15gx/GaNksBNh+porY5ytxxuW7jh1NxJeRnhrj/pb2c991VPL+jjJMNrSya5Dx+8exCfvZ35/LPNyxk6phswkHDnvKeB9n+inq+9uhmKuqa2Xi4iuXfX813/7q90+daa5OBdOBEPbVNrVQ3poYxm1pjrNpWmgyN49WpX+SHO9kKLd3Gw9WsP1jV4+PuytuHnO9xtKqRn67ew7ef2MpfNx3jnLufpawXG2T3129fO8hV97yYHBqXkanPzR7iLbedN5Ur5hczYZRTIRS4FVlJdRPzx+dhjGF6YQ5fuGYe4AyTzSjM4ZI5TpUysSCL/7jprOT3++D50/i/z+3m/Jljkp2UE0ZlEgwY8jLb/md17rTRPHLnhdyzchcfPH9am+3DFk8p4MmNJVQ3tnLoRAPXLBrPe8+ZzJ2/W8/CCfkUZEe4cfHE5JUBRudEmDI6m9eoZGZRDpfNK+Le1XtoaIkxLj+Db/xlC1UNrbx/ySRuXTaV57aXMm98frLqO2NSfnKIFpz5xARjDCv/6TKywsHk8OrWkprk3NXfthxLBjvADT9ZkxwyzM8MsWaPc4zvOmsiP1y5i5ZYnJlFuWSGy2mJxvm7C6bxwKsHCIcCfPSBdUSCAS6alVpWmf5zmTY2p8cV2Z6yOlbc8yIAs4ty+cnzu6lqaOUvG47ytesXJOdOK+qauesPb7PuwEnet2QS4FSRd/5uPS3ROP9z50W0xuJ8+vdvsWp7Gf/n/WeRnxXqdJPqzlhrKa9tpqaxFWtthxGFlmicSBf7n7ZE46zdd4JL5xRijOHttLm7Hz67E2MMhysbqGmK8tU/b2bH8Vr+6/alBAOGiQVZbZZ7DKQ9ZXU0tMQ4crKBOePyuv8CGRaqyE4TgYBJhhikFllDqqkkXWFuBqu/sDw5zNXeJy+fxW8/uoz541MVRygYYHx+JvlZ4Q7PP2fqaH770fOTFWHCTedOprY5yvef2UE0bplZmMPVC8fxkYtncPN5UwDa7FM4JjuSXJYwuziXxVMKyIkEmT8+j89fPY8jJxupa44ye1weCybks/lb1/CxS2YAMCor3GGor72cjBCBgGFWcQ6hgOF3rx2ksr6F6WOzeWlXRZvlAVPSfm5TxmRTUddMOGi45bwpLJjgXPn7XWdPoCgvg2ljc/j81fPY8C9X86NbFjOzMIdff+Q8ZhfndnYYzC7K7XFF9pu1B5IB8fC6w1Q1tLJiQTEVdS1sSmvm+eObh3llzwlaYvHkEg+AV/ac4K1DVdQ3R3n0raOs2l7GqKww331qO3f+7i0eWHuQsTkRivIyOHLSWarx8JuHOxxHdWMrLbE4tc3RNhUeOCG6+O5nWbmttMPXAfx09R5u/9UbrHaXeWw4VMXCCc5/W3HrDCmucpckPLejjKNVjWw4XMXVP3qJJXevbNPE01dNrbEOw7GJTbnTL50kI4+C7DSVFQkmP75rxZxef31GKJhsi09352UzuXnplB5/n3Onjeb8GWN48HVnfmtmUS6BgOGb71rIue5GynPH5SV/USeGFsFZHxcOBvjxrefwf246ixULxuEWH8x2q8RwMJD8hXjGpPxuG2PSz2/S6CzWHTxJdiTI19+5kJZYnCc2lgDw0hcv5/nPX0YoYBifn5kcCj5nymiK8zP5212X8PuPXcCiiaM4e3IBF80aSzBgCAcDXD6vmOe/sLxNNdberOIcDp5oOOUv6NqmVl7fd4K/vH2U684Yz+XzipKV5GdXzMUY+PYTW5OL2f+66RjnTC1g3ri8NpcFAicoNh6uYs2eCorzMvj69QvahFFRXgZTRmdxoKKBbz++lS/9aVOHjseytIXfR042UlHXzCo3uNYdqKShJcaaThacl9U0cf9L+wD47dqDbD9Ww/bjNVyzaDzj8jPICAWS7/9N505m4qhMgOQ+oy2xOPe6Hbmncv9Le5PvX3sNLVEu+Pfn+J92lzY6VuUMY/ZkLrE/frVm/4i7QK6XaGjxNPbCF5YzOieSbMUfCB9yt8Tqja+/cwE/e2Evs4tzOXtyx13iE2G04XAVY3IizCh0QioxzHdV2nqr86aP4fX9lcwZl6p0po/NoTgvI9l92VMTR2Vx8EQD99y8OBmqr+07QShgmDQ6C2MMG/7lamIxy91POhd9uGh2x9e4938t6dXrApw1uYBY3PLc9lKe2FTCF6+ZT21TKz9/cR+zinP5zBWz+c8X9vIzt2PzlqVTeG1/Jat3llOcl8Giifksc38Wbx+q4szJo9h2rIZvvHMBu0pr2XG8lllFOW06I9cdPMlr+05w/syxvOecSZyobyEYgH97agdHqxr5h4tnJOcqAX7z6gEW37I4+XlZTSrIdhyv5UuPbGLbsRpe/tLlyaHCxHKPu5/Yxr6KOr55w0Ke2HiMpmiM9yyeyGMbSzhyspGCrDC3XzSNaDxO3FrePlTFq3tP8PFLZ/KDD5zNwm8+zfZjqe7aF3aW8U9XzeWJjSU8+PpBfv+PFySHVMEZ9rz3+T0snJjPu86e2OHnva2khqqGVt46eJKbl07hnpW7OFnfQkmyIut6SPWpzcd4eXcF//6+Mzs8VtvUSkYo2OWQauLYfrRyF+fNGMNlc0+/fWYHgoLsNDa9sON2V8PhrMkF/Ozvzj3lc86ZWsDesjqyI0HOnjyKBz6yLNllmO6j75hBUV4GY9Ou4RYIGFZ93pn76o3vvf9MDpxoSP5yGZMTobK+hUkFWQTdX5KJuZnCPOf1Lu7kmPri4tmFhIOGr/9lC5X1Lew8XsvBEw1EQgH+uvkY5bVNHHO7Ij/yjhlcMHMsDW5DwpKpozHGcN/fncvhkw287z9f5VMPvkUwYLjuzAlkhoM8vO4IZ0waRXltM2NzM4gEnR1lymqbuWDmGCKhAJ9YPou65ij/9tQOZhXl8qnLZ7FqWyn7K+pZsXAcT246xjfftTDZOFRel2rC+PYTW5NXIH95dwUb3OaNbcdqnPm4dYepbY6yp+wNxuZmcNakUXz9nQs5VNnAhsNVfO/9Z1GQHeHzVztztn9af4RY3CaHYgtzM9hW4oTi4ikFbC2ppjka4+ktx3ltXyWHTzYkm3gASmuaqWmKdnkB2kTA7i2vIx63/P71g9Q1R5OV6+odZfzprSP84sNLOc9dd5jw+9cPsWZPBd+8YWGbkQ6Am362lnfMKeSfb+jsUo2OqoZWapuj7OtkKHl/RT3Tx2b3eCThdKWhRfGEz66Yy8N3XogxztKDy+YWJcMk3dWLxnPv/1rS4X/8/Mww4WDv/nOfNjanzV/Is9OaWtq7YOZYls0Yw9ldzCn2Vm5GiGUzxlBZ38KorDB7y+uZXZzL2q9cyQ1nTWDV9jL2lNWxZNpoPvqOGQQChrOmjCIUMJw/0/lFOzonwlmTC7jmjPHUt8T41rsXMakgK3mMkwqyuHHxJG49bwrL5xUlr0qeXrnmZoR44tPv4BcfXkpGKMhDd1zAk595B7eeN4WWmLNgPSFRkUVCAWqborz77IlMHJXJ6p1lbD5aTWFuBk2tcZ7Zepza5ihXzi/myMlGNh6u4rK5RRTlZfDnT17M7u9e32F4+v3nTuaPH78w+b4W5WUkO1JXLCimNWbZdbyOrSWpRfVX/vCF5ILqHe7uMseqm3h8Ywn/+uS25NXYAbYcdR7fW17PztJaKupakiFmDOyrqKeqoZWvP7q5zQL8aCzOW4dOul9bx+60C9dGY3F2l9WyLq2zdf3Bk9zxm3XJ/TWPVTey3x0iPXyykeZoqjvy+R2lXP6DF/j8wxu559mdHK1q5Oafr+Vfn9yGtZbvPLmNG+9d0+Y8TleqyMQTRmWFB3QItC9mj8vljQOVjO8kyC6fV8zl84oH9PUun1fMK3tO8C/vWkgkFOCCmWMZlR1mydTRPLnJubrBB85N/cIvznPm5tpX2ne/exHvXzKJK+Y7Q7DzJ+SxYsE4rphfzFK3umiNxZkwKpNDlY0dNqY+M224N/E+JDpTd5XWJqvQstpmssJBphfmsP1YDbeeN5WscJCH1x0mbuEjF8/g3tV7eODVAwB86dr5rD90kqqGVi5N+4Ohsz9Q2ivMTVXcV8wfxw+e3cWreyuSe4Pe9+JemqNx/v2pHXx2xdw2gfuvT25zjjUS5HMr5vLS7nLedsOosr6Fx9vNoy2ckM/WkhomjspkV2kdP3thLzedO5n/enk/sXg8WQn/eNUunttRxgtfWM60sTlU1LUQt7CztJZoLE4wYPjt2gM8u62Unzy3h/eeM4l3/uRlLnJ/fs48ZTUTRmUyZUw2r6dtOACwdt8J3jxwkm0lNQSDhl+617nbW1532ndUKshEeuhUFdlg+MDSKbTGLDecNbHNHEt6sLTveuzsF9rY3IxkiIEz5/hfty9t85xwMMDfXzyjx8dWlJtBQXaYtw5V8ezW1/jcVXMpr22mOD+DBePzaI3FuWDmGFpicR5ed5j3LJ7IJy+fxZ/fOsK6gycZkxNh7rhcbj1vKo++fYTFUwp6/NpAsrkmKxxkwYQ8CrLD/HGd00lpDDRH44SDhud2lPHcjrI226eV1TYTCQX46eo9jMvP5Bt/2QI4zUBbjtbwyzX7mVGYw7HqRppa4yybMYatJTV8+br5PLe9jP/73G7ufX4PLe22Rkt0Ve4urWPa2Jxkx2NTa5y/++XrBIxJ/jH2qzX7WXegkmjc8lJak8dHf/0m+Vlh1nz5crYerWHhhHz+5V0LefD1Q8mArWuO8vMX97Fs+hjeOFDJ24ermDMuD2stf9tynAtnjmV02tD66UBDiyI9lAiN9GUMg2lUVphPLJ/VoVFg4YR8EiOns4qHZ57TGMOc4lz+uqmEtftO8Lk/bmBfRR3FeRn863vP4E+fuCg5BLz9O9fy41vPITsS4t/f76xFXDylAGMMX7xmHs9/fjmhXg77JoKsKM/ZU/T8GWOSW5wlumk/u2Iun7p8FlfOd4Ye0xuJPnj+VOIW/u9zu4m4GxB87BJny7SWaJybzp3MAvfn/MHzp/G+JZO4auE4vvOeM1gytYD3nzuJpz5zCePzM5k8Oit57UCAg+6i8dK0hduv7avkzQOV7Cqt5ezJo5jsdsQmis/EXGttc5SjVY3sLqtj89Fqzpo8ivNnjuUfLp4OwIUzxzJxVCZ5GSHu/eA55GWGkt2jz24r5ZMPvsV/v3L6XZFcFZlID501eRRzinM7TPYPtZyMELOKctlXXseMYWzYmTMujzcPnCQvI8Sx6kaOVjVy5fxisiNtf61khFINEJfNLeKem89mTrFTOQYDhpw+LGYuzEsFGcB3bjwjeQWG684Yz5rd5dxw1gSmjc3hRF0zV/3oJZbPK+ZgZQNVDa185OIZPLL+COW1zVyzaBw//9BS4nHLXX/YAMDHLplJSzROU2uc2cW53HPz4uRr/8+dFyU/fvBj59PYEuOelbuSjSSH3DmvY9VtdyBpjVn2ltfzkYtn8Inls/j1q07g/HT1XuaOy+Xwycbktet+//ohqhtbk2soF08p4OOXzeTyecWEg4ZozFKcl8nZkwt4aVc5//THDbzibvb9xgBeeLYvqhqc+cXOhuAHi4JMpIcKsiOsdC+RM9wunDmW7EiwTUgMtbluhXrD2RO5/szx3P3ENpbP736e8H1LJvf7tYvcObLENfmK8zN59StXUFnfwvzxeSybMSbZtTg2N4MXvric7HCQF3aVkxVuYsqYbC6aNZZntpbyDreCCwQMv//H85lWmEMkFOCzK+Zw15WnXmM5yx1unlWUw/M7nCsoJCqy49VNREIBZhbmcKy6Kbkub3phNkV5GXzxmvm8fegkP129l6ljsinOyyQat+yrqOMP7r6hZ7pBZozhq9ct6PD6i6cUsGZPBdWNpeREQpw7bTQbDledcheV/mhqjVFe28yUMdlUN7Ryz8qd1DZF+dxVc5MbBHz1z5t5bkcZ//H+M3nvOf1/r3tCQSbiQf98w8Ih30C3vbPcea3rzxzPJXOKhjTk04cWEyYWZDHR3bllVlHbucPERtZ3XTk72Y145YJxrNpexmVpC/svSls+4XTI9ux43nPOJGJxOFrVwO7SOjYeruLIyUbG52fyo1sWYwy8+95XaInG2ywLWDRxFIW5Ec6YNIqPXDyDuLV87287+K81+zl/xhgWTMg/xavCbe6eoHdcOpPC3Az+tvkYn3jwLbaUVLNk6uieHTx0uqVYZ770yCae3nqcxz51Mfe/tI/HN5ZggLzMEN++8Qystby+v5LWWJwvP7KZqxeO71PF3VsKMhEPGoy/tntrydTRvPCF5cOyHjERZMV5vdtcO73p5aYlk7lw5tg2W4311aKJo1g0cRT//rftPLO1lBt/+goAy6anwmjeuDw2H61mRlqQRUIBXvzi5WSGgwQChgCGf7p6LrecN4XZxbndhsukgiy+dn2qUkt0of5qzX5mvicHa52tw9LfI2stv371ACsWjGPKmGy+9MhGymqb+fU/LAOc59/7/G4OnGjga9cvYEZhDq/tO8GJuhae2nyMaNxy831rqW2O8pkr57CvvI7HN5Ywd3we+ZlhKutbeNfZE3liYwmv7TvBlQsG/wKxCjIR6bPhWlQ/aXQWKxYUJ4cF+yIQMAMSYummjWn78xidk1oysnBCPjuP1zKxoO3cUfuKJTsS6nM7fVFeBn9/0XQeWHuAWNwSCgZ4dU8F3//AWfzv37/NoomjuGROIT9cuYtdpbV89z1n8uy2UqobW6msbyEvM8QnH1zP6/sqiVnLggn5fG7FHD7z0NvJLch+dMvZrN5RzozCHD55+Sxe3XOCJzcd4+uPbkk2r9xxyUxWbSvlpV3lXDKnaND/8FKQiYjnOEsIzhvuw+ggsQ/oWZNHselINfXNqQXOn75iNtecMa7XHZq99a13LyJuLQ+vO0w4EKC2Ocp3/7qdQMCwt7wu2Qzy9JbjfPD8aVQ1OHN3L+4qY+fxOl7Zc4IffOBsHnj1AK/tO8Gu0gmU1TazaGI+i6cU8N5zJreZ+7pkTiFXLRxHOGh4avNx8jJCLJqYzwUzx/Dnt4/y2MYSnr7r0kFt/lCQiYgMkItnFfLL25dy2dwi7n95HyvShtWmjMke8AqwK1cuGMdv1h6kCWc+cG95PTedO5mPXTKTbz62hXOnjeY/X9jLj1c5e2fmRIL85+q97Cmv47ZlU7np3MnsPF7DA2sP8twOpxv0/g8v7fTqEaFggF98eCmxuGVf+ctMHp1NIGC44ayJrNlTwTvPnNBmN5TBYE51FdnhsnTpUrtu3brhPgwREU9qao2x5DsraY3FmTomm73l9fzsg0u4zr1CfFNrjGXfXUVNU5RJBVlcNq+I379+iGUzxvDff38eORkhVm0r5R9/s47C3Ah5mWFWf2F5t69b09RK0DhLKqy1yeHNgWCMWW+tXdrZY6rIRER8JjMc5H1LJtHQHGPa2Bzuf2kv75hT2ObxB//xAu5+cisXzy7k45fO4pPLZ7W56Ox5M5zNoyvqWnjP4kk9et1Edyg4XZ+h4NBsdqyKTETEx1qicU42tDAuv/dzVPvK62iJxZlTnNejPTAHkyoyEZHTVCQU6FOIgXOhWy8Y/sUoIiIi/aAgExERT1OQiYiIpynIRETE0xRkIiLiaQoyERHxNAWZiIh4moJMREQ8TUEmIiKepiATERFPU5CJiIinKchERMTTFGQiIuJpCjIREfE0BZmIiHjaiLywpjGmHDjYz29TCFQMwOGMVDo/7/LzuYHOz8tG8rlNs9YWdfbAiAyygWCMWdfV1UT9QOfnXX4+N9D5eZlXz01DiyIi4mkKMhER8TQ/B9n9w30Ag0zn511+PjfQ+XmZJ8/Nt3NkIiJyevBzRSYiIqcBBZmIiHiaL4PMGHOtMWanMWaPMeYrw308/WWMOWCM2WyM2WCMWefeN8YYs9IYs9u9HT3cx9lTxphfGWPKjDFb0u7r8nyMMV9138udxphrhueoe66L8/uWMeao+x5uMMZcn/aYZ87PGDPFGLPaGLPdGLPVGHOXe78v3r9TnJ/n3z9jTKYx5g1jzEb33L7t3u/9985a66t/QBDYC8wEIsBGYOFwH1c/z+kAUNjuvv8DfMX9+CvAfwz3cfbifC4FlgBbujsfYKH7HmYAM9z3Njjc59CH8/sW8IVOnuup8wMmAEvcj/OAXe45+OL9O8X5ef79AwyQ634cBl4HLvDDe+fHimwZsMdau89a2wL8AbhxmI9pMNwIPOB+/ADwnuE7lN6x1r4EVLa7u6vzuRH4g7W22Vq7H9iD8x6PWF2cX1c8dX7W2mPW2rfcj2uB7cAkfPL+neL8uuKZ87OOOvfTsPvP4oP3zo9BNgk4nPb5EU79H6IXWOBZY8x6Y8wd7n3jrLXHwPmfDygetqMbGF2dj5/ez08bYza5Q4+J4RvPnp8xZjpwDs5f9r57/9qdH/jg/TPGBI0xG4AyYKW11hfvnR+DzHRyn9fXGFxsrV0CXAd8yhhz6XAf0BDyy/v5M2AWsBg4BvzQvd+T52eMyQX+BHzWWltzqqd2cp8Xz88X75+1NmatXQxMBpYZY844xdM9c25+DLIjwJS0zycDJcN0LAPCWlvi3pYBj+KU96XGmAkA7m3Z8B3hgOjqfHzxflprS91fInHgF6SGaDx3fsaYMM4v+QettX927/bN+9fZ+fnp/QOw1lYBLwDX4oP3zo9B9iYwxxgzwxgTAW4FHh/mY+ozY0yOMSYv8TFwNbAF55xud592O/DY8BzhgOnqfB4HbjXGZBhjZgBzgDeG4fj6JfGLwvVenPcQPHZ+xhgD/BLYbq29J+0hX7x/XZ2fH94/Y0yRMabA/TgLWAHswA/v3XB3mwzGP+B6nG6jvcDXh/t4+nkuM3E6hzYCWxPnA4wFngN2u7djhvtYe3FOD+EMz7Ti/NX30VOdD/B1973cCVw33Mffx/P7LbAZ2ITzC2KCF88PeAfO8NImYIP773q/vH+nOD/Pv3/AWcDb7jlsAb7p3u/5905bVImIiKf5cWhRREROIwoyERHxNAWZiIh4moJMREQ8TUEmIiKepiATERFPU5CJiIin/X+XrQ0q0ZHwJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--- set up ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('using gpu')\n",
    "else:\n",
    "    print('using cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# OPTIMIZERS & REGURALIZATION\n",
    "\n",
    "# EARLY STOP AFTER 8th EPOCH, when train acc about 98.6%, AVG Test acc 96.15%\n",
    "optimizer_1 = optim.Adam(model.parameters(), lr=LR, weight_decay=0.05)\n",
    "\n",
    "# EARLY STOP AFTER 3rd EPOCH, when train acc about 92.86%, AVG Test acc 82.77%\n",
    "optimizer_2 = optim.RMSprop(model.parameters(), lr=LR, weight_decay=0.1)\n",
    "\n",
    "# RUNS THROUGH ALL 10 EPOCHS, final train acc about 95.36%, AVG Test acc 91.26%\n",
    "optimizer_3 = optim.SGD(model.parameters(), lr=LR, momentum=0.5)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optimizer_1\n",
    "\n",
    "#--- TRAINING WITH DEV SET ---\n",
    "\n",
    "previous_dev_loss = 1000000\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    dev_loss = 0\n",
    "    dev_correct = 0\n",
    "    total = 0\n",
    "    for batch_num, (data, target) in enumerate(dev_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "                \n",
    "        # Compute prediction error\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "\n",
    "        dev_loss += loss.item()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        total += len(target)\n",
    "        dev_correct += torch.sum(pred.argmax(1) == target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Dev Acc: %.3f%% (%d/%d)' % \n",
    "              (epoch, batch_num, len(dev_loader), dev_loss / (batch_num + 1), \n",
    "               100. * dev_correct / total, dev_correct, total))\n",
    "    \n",
    "    print(\"Epoch train loss: \" + str(dev_loss))\n",
    "    print(\"Previous epoch train loss: \" + str(previous_dev_loss))\n",
    "    \n",
    "    # EARLY STOPPING\n",
    "    if dev_loss > previous_dev_loss:\n",
    "        print('early stopping')\n",
    "        break\n",
    "        \n",
    "    previous_dev_loss = dev_loss\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(batch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 0/127: Loss: 0.0000 | Test Acc: 30.469% (39/128)\n",
      "Evaluating: Batch 1/127: Loss: 0.0000 | Test Acc: 27.734% (71/256)\n",
      "Evaluating: Batch 2/127: Loss: 0.0000 | Test Acc: 28.646% (110/384)\n",
      "Evaluating: Batch 3/127: Loss: 0.0000 | Test Acc: 26.953% (138/512)\n",
      "Evaluating: Batch 4/127: Loss: 0.0000 | Test Acc: 26.250% (168/640)\n",
      "Evaluating: Batch 5/127: Loss: 0.0000 | Test Acc: 26.693% (205/768)\n",
      "Evaluating: Batch 6/127: Loss: 0.0000 | Test Acc: 27.567% (247/896)\n",
      "Evaluating: Batch 7/127: Loss: 0.0000 | Test Acc: 27.637% (283/1024)\n",
      "Evaluating: Batch 8/127: Loss: 0.0000 | Test Acc: 26.823% (309/1152)\n",
      "Evaluating: Batch 9/127: Loss: 0.0000 | Test Acc: 27.422% (351/1280)\n",
      "Evaluating: Batch 10/127: Loss: 0.0000 | Test Acc: 27.557% (388/1408)\n",
      "Evaluating: Batch 11/127: Loss: 0.0000 | Test Acc: 27.474% (422/1536)\n",
      "Evaluating: Batch 12/127: Loss: 0.0000 | Test Acc: 27.704% (461/1664)\n",
      "Evaluating: Batch 13/127: Loss: 0.0000 | Test Acc: 27.679% (496/1792)\n",
      "Evaluating: Batch 14/127: Loss: 0.0000 | Test Acc: 28.073% (539/1920)\n",
      "Evaluating: Batch 15/127: Loss: 0.0000 | Test Acc: 28.613% (586/2048)\n",
      "Evaluating: Batch 16/127: Loss: 0.0000 | Test Acc: 28.493% (620/2176)\n",
      "Evaluating: Batch 17/127: Loss: 0.0000 | Test Acc: 28.863% (665/2304)\n",
      "Evaluating: Batch 18/127: Loss: 0.0000 | Test Acc: 28.701% (698/2432)\n",
      "Evaluating: Batch 19/127: Loss: 0.0000 | Test Acc: 28.945% (741/2560)\n",
      "Evaluating: Batch 20/127: Loss: 0.0000 | Test Acc: 28.943% (778/2688)\n",
      "Evaluating: Batch 21/127: Loss: 0.0000 | Test Acc: 28.835% (812/2816)\n",
      "Evaluating: Batch 22/127: Loss: 0.0000 | Test Acc: 28.974% (853/2944)\n",
      "Evaluating: Batch 23/127: Loss: 0.0000 | Test Acc: 29.134% (895/3072)\n",
      "Evaluating: Batch 24/127: Loss: 0.0000 | Test Acc: 29.062% (930/3200)\n",
      "Evaluating: Batch 25/127: Loss: 0.0000 | Test Acc: 28.696% (955/3328)\n",
      "Evaluating: Batch 26/127: Loss: 0.0000 | Test Acc: 28.472% (984/3456)\n",
      "Evaluating: Batch 27/127: Loss: 0.0000 | Test Acc: 28.404% (1018/3584)\n",
      "Evaluating: Batch 28/127: Loss: 0.0000 | Test Acc: 28.314% (1051/3712)\n",
      "Evaluating: Batch 29/127: Loss: 0.0000 | Test Acc: 28.229% (1084/3840)\n",
      "Evaluating: Batch 30/127: Loss: 0.0000 | Test Acc: 28.251% (1121/3968)\n",
      "Evaluating: Batch 31/127: Loss: 0.0000 | Test Acc: 28.320% (1160/4096)\n",
      "Evaluating: Batch 32/127: Loss: 0.0000 | Test Acc: 28.243% (1193/4224)\n",
      "Evaluating: Batch 33/127: Loss: 0.0000 | Test Acc: 27.964% (1217/4352)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-a66b3b450bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mstartpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistortion_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mperspective\u001b[0;34m(img, startpoints, endpoints, interpolation, fill)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mperspective\u001b[0;34m(img, perspective_coeffs, interpolation, fill)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'5.0.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPERSPECTIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperspective_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[1;32m   2424\u001b[0m                 \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQUAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillcolor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m             im.__transformer(\n\u001b[0m\u001b[1;32m   2427\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillcolor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m             )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__transformer\u001b[0;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[1;32m   2501\u001b[0m             \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2503\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "#--- train ---\n",
    "train_loss = 0\n",
    "train_correct = 0\n",
    "total = 0\n",
    "\n",
    "train_acc_all = 0\n",
    "\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        total += len(target)\n",
    "        train_correct += torch.sum(pred.argmax(1) == target)\n",
    "        preds.extend(pred.argmax(1).cpu().numpy())\n",
    "        targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        test_acc_all += (test_correct / total)\n",
    "\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num, len(train_loader), train_loss / (batch_num + 1), \n",
    "               100. * train_correct / total, train_correct, total))\n",
    "\n",
    "    cf_matrix = confusion_matrix(targets, preds)\n",
    "    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix) * 1000, \n",
    "                         index = [i for i in annotation_dict_train.keys()],\n",
    "                         columns=[i for i in annotation_dict_train.keys()])\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "    \n",
    "print('AVG test acc:' + str(test_acc_all*100/len(test_loader)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
