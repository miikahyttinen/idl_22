{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse file nums to dicts\n",
    "images_file_path = os.getcwd() + '/../data/images'\n",
    "image_ids = []\n",
    "for file in os.listdir(images_file_path):\n",
    "    filename = os.fsdecode(file)\n",
    "    image_ids.append(filename[2:-4])\n",
    "\n",
    "train_ids, dev_ids = train_test_split(image_ids, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE_TRAIN = 128\n",
    "BATCH_SIZE_DEV = 16\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 14\n",
    "DATA_DIR = os.getcwd() + '/../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class ProjectDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotation_folder, selected_keys=None, transform=None, target_transform=None):\n",
    "        data = {key: {'file': f'im{key}.jpg', 'labels': []} for key in selected_keys}\n",
    "        self.data = []\n",
    "        self.classes = []\n",
    "\n",
    "        for file in os.listdir(annotation_folder):\n",
    "            filename = os.fsdecode(file)\n",
    "            cls = filename.split('.')[0]\n",
    "            self.classes.append(cls)\n",
    "            file_path = os.path.join(annotation_folder, filename)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    idx = line.replace('\\n', '')\n",
    "                    if not selected_keys or idx in selected_keys:\n",
    "                        data[idx]['labels'].append(cls)\n",
    "        \n",
    "        for key, val in data.items():\n",
    "            filename = val['file']\n",
    "            labels = torch.Tensor([x in val['labels'] for x in self.classes])\n",
    "            self.data.append({'file': filename, 'labels': labels})\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        data = self.data[item]\n",
    "        labels = data['labels']\n",
    "        image_path = os.path.join(self.image_folder, data['file'])\n",
    "        image = torchvision.io.read_image(image_path, torchvision.io.ImageReadMode.RGB)\n",
    "        image = image.type(torch.FloatTensor)\n",
    "        image = torch.div(image, 255)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "            \n",
    "        return image, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7841 0.4900625\n",
      "1983 0.49575\n"
     ]
    }
   ],
   "source": [
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      #transforms.RandomPerspective(distortion_scale=0.2),\n",
    "                                      #transforms.ColorJitter(),\n",
    "                                      #transforms.ToTensor()\n",
    "                                     ])\n",
    "dev_transform = transforms.Compose([])\n",
    "\n",
    "annotation_path = DATA_DIR + 'annotations'\n",
    "image_path = DATA_DIR + 'images'\n",
    "train_set = ProjectDataset(image_path, annotation_path, selected_keys=train_ids, transform=train_transform)\n",
    "dev_set = ProjectDataset(image_path, annotation_path, selected_keys=dev_ids, transform=dev_transform)\n",
    "# test_set  = datasets.ImageFolder(DATA_DIR % 'test',  transform=test_transform)\n",
    "\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_set, batch_size=BATCH_SIZE_DEV, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(32, 16, kernel_size=5, padding=2),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2)\n",
    "#         )\n",
    "        #self.fc = nn.Linear(4096, NUM_CLASSES)\n",
    "        self.fc = nn.Linear(32768, NUM_CLASSES)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        #out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_metrics(targets, preds):\n",
    "    t = np.asarray(targets)\n",
    "    p = np.asarray(preds)\n",
    "    false_neg = np.sum(np.logical_and(p == 0, t == 1))\n",
    "    false_pos = np.sum(np.logical_and(p == 1, t == 0))\n",
    "    true_neg = np.sum(np.logical_and(p == 0, t == 0))\n",
    "    true_pos = np.sum(np.logical_and(p == 1, t == 1))\n",
    "    \n",
    "    recall = true_pos / (true_pos + false_pos)\n",
    "    precision = true_pos / (true_pos + false_neg)\n",
    "    f1_score = 2 * (precision*recall / (precision+recall))\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n",
      " Training: Epoch 0 - Batch 124/125: Loss: 0.2270 ---\n",
      "Precision:  0.3359249660619524\n",
      "Recall:  0.24691582002902757\n",
      "F1 Score:  0.28462383018769283\n",
      "---\n",
      "\n",
      " Dev: Epoch 0 - Batch 249/250: Loss: 0.2085ev: Epoch 0 - Batch 86/250: Loss: 0.2099\n",
      "Epoch dev loss: 52.135169081389904\n",
      "Previous epoch dev loss: 1000000\n",
      "\n",
      "---\n",
      "Precision:  0.31732205077152814\n",
      "Recall:  0.31930879038317056\n",
      "F1 Score:  0.3183123205592311\n",
      "---\n",
      "\n",
      " Training: Epoch 1 - Batch 124/125: Loss: 0.2063 ---\n",
      "Precision:  0.3313587560162903\n",
      "Recall:  0.3312769895126465\n",
      "F1 Score:  0.3313178677196446\n",
      "---\n",
      "\n",
      " Dev: Epoch 1 - Batch 249/250: Loss: 0.2035ev: Epoch 1 - Batch 30/250: Loss: 0.1998\n",
      "Epoch dev loss: 50.87595587968826\n",
      "Previous epoch dev loss: 52.135169081389904\n",
      "\n",
      "---\n",
      "Precision:  0.320059731209557\n",
      "Recall:  0.33350622406639\n",
      "F1 Score:  0.3266446532893066\n",
      "---\n",
      "\n",
      " Training: Epoch 2 - Batch 124/125: Loss: 0.2021 ---\n",
      "Precision:  0.34888312970504753\n",
      "Recall:  0.3409927024908027\n",
      "F1 Score:  0.34489279287522495\n",
      "---\n",
      "\n",
      " Dev: Epoch 2 - Batch 249/250: Loss: 0.2005\n",
      "Epoch dev loss: 50.130713015794754\n",
      "Previous epoch dev loss: 50.87595587968826\n",
      "\n",
      "---\n",
      "Precision:  0.3668491786958686\n",
      "Recall:  0.3243124312431243\n",
      "F1 Score:  0.34427186733621395\n",
      "---\n",
      "\n",
      " Training: Epoch 3 - Batch 124/125: Loss: 0.1992 ---\n",
      "Precision:  0.36955448599284213\n",
      "Recall:  0.3467863346844239\n",
      "F1 Score:  0.35780857928067866\n",
      "---\n",
      "\n",
      " Dev: Epoch 3 - Batch 249/250: Loss: 0.1982ev: Epoch 3 - Batch 39/250: Loss: 0.1934\n",
      "Epoch dev loss: 49.54410033673048\n",
      "Previous epoch dev loss: 50.130713015794754\n",
      "\n",
      "---\n",
      "Precision:  0.34843205574912894\n",
      "Recall:  0.34154671871188097\n",
      "F1 Score:  0.34495503264752986\n",
      "---\n",
      "\n",
      " Training: Epoch 4 - Batch 124/125: Loss: 0.1971 ---\n",
      "Precision:  0.3824509440947797\n",
      "Recall:  0.35320264417597447\n",
      "F1 Score:  0.36724536351247256\n",
      "---\n",
      "\n",
      " Dev: Epoch 4 - Batch 249/250: Loss: 0.1968\n",
      "Epoch dev loss: 49.189354829490185\n",
      "Previous epoch dev loss: 49.54410033673048\n",
      "\n",
      "---\n",
      "Precision:  0.39522150323544053\n",
      "Recall:  0.3395338892452427\n",
      "F1 Score:  0.36526739505462913\n",
      "---\n",
      "\n",
      " Training: Epoch 5 - Batch 124/125: Loss: 0.1951 ---\n",
      "Precision:  0.39417499691472296\n",
      "Recall:  0.35936093609360936\n",
      "F1 Score:  0.3759637455123301\n",
      "---\n",
      "\n",
      " Dev: Epoch 5 - Batch 249/250: Loss: 0.1954\n",
      "Epoch dev loss: 48.83875238895416\n",
      "Previous epoch dev loss: 49.189354829490185\n",
      "\n",
      "---\n",
      "Precision:  0.42807366849178696\n",
      "Recall:  0.33038801383019595\n",
      "F1 Score:  0.37294015611448394\n",
      "---\n",
      "\n",
      " Training: Epoch 6 - Batch 124/125: Loss: 0.1938 ---\n",
      "Precision:  0.40645439960508456\n",
      "Recall:  0.36020123585060426\n",
      "F1 Score:  0.3819325660278897\n",
      "---\n",
      "\n",
      " Dev: Epoch 6 - Batch 249/250: Loss: 0.1944\n",
      "Epoch dev loss: 48.605625584721565\n",
      "Previous epoch dev loss: 48.83875238895416\n",
      "\n",
      "---\n",
      "Precision:  0.37779990044798406\n",
      "Recall:  0.3518776077885953\n",
      "F1 Score:  0.3643783005280845\n",
      "---\n",
      "\n",
      " Training: Epoch 7 - Batch 124/125: Loss: 0.1922 ---\n",
      "Precision:  0.4157719363198815\n",
      "Recall:  0.3664346312812704\n",
      "F1 Score:  0.3895473203445684\n",
      "---\n",
      "\n",
      " Dev: Epoch 7 - Batch 249/250: Loss: 0.1932\n",
      "Epoch dev loss: 48.30003283172846\n",
      "Previous epoch dev loss: 48.605625584721565\n",
      "\n",
      "---\n",
      "Precision:  0.3904927824788452\n",
      "Recall:  0.36294240111034004\n",
      "F1 Score:  0.37621388322743077\n",
      "---\n",
      "\n",
      " Training: Epoch 8 - Batch 124/125: Loss: 0.1913 ---\n",
      "Precision:  0.42428730099962975\n",
      "Recall:  0.3747343179464821\n",
      "F1 Score:  0.39797424395890607\n",
      "---\n",
      "\n",
      " Dev: Epoch 8 - Batch 248/250: Loss: 0.1926Dev: Epoch 8 - Batch 249/250: Loss: 0.1925\n",
      "Epoch dev loss: 48.132433749735355\n",
      "Previous epoch dev loss: 48.30003283172846\n",
      "\n",
      "---\n",
      "Precision:  0.42309606769537084\n",
      "Recall:  0.3499382461918485\n",
      "F1 Score:  0.3830554303740424\n",
      "---\n",
      "\n",
      " Training: Epoch 9 - Batch 124/125: Loss: 0.1898 ---\n",
      "Precision:  0.4343453042083179\n",
      "Recall:  0.37866480176448436\n",
      "F1 Score:  0.4045983618335967\n",
      "---\n",
      "\n",
      " Dev: Epoch 9 - Batch 249/250: Loss: 0.1916\n",
      "Epoch dev loss: 47.89781206846237\n",
      "Previous epoch dev loss: 48.132433749735355\n",
      "\n",
      "---\n",
      "Precision:  0.3964659034345446\n",
      "Recall:  0.3688353785598518\n",
      "F1 Score:  0.3821518531845988\n",
      "---\n",
      "\n",
      " Training: Epoch 10 - Batch 124/125: Loss: 0.1885 ---\n",
      "Precision:  0.4380476366777737\n",
      "Recall:  0.3818719741796665\n",
      "F1 Score:  0.4080354063685481\n",
      "---\n",
      "\n",
      " Dev: Epoch 10 - Batch 249/250: Loss: 0.1909\n",
      "Epoch dev loss: 47.72579116374254\n",
      "Previous epoch dev loss: 47.89781206846237\n",
      "\n",
      "---\n",
      "Precision:  0.42085614733698357\n",
      "Recall:  0.3594048884165781\n",
      "F1 Score:  0.38771065000573196\n",
      "---\n",
      "\n",
      " Training: Epoch 11 - Batch 124/125: Loss: 0.1876 ---\n",
      "Precision:  0.4471800567690979\n",
      "Recall:  0.3849054599532611\n",
      "F1 Score:  0.413712393674716\n",
      "---\n",
      "\n",
      " Dev: Epoch 11 - Batch 249/250: Loss: 0.1898Dev: Epoch 11 - Batch 152/250: Loss: 0.1875\n",
      "Epoch dev loss: 47.462187692523\n",
      "Previous epoch dev loss: 47.72579116374254\n",
      "\n",
      "---\n",
      "Precision:  0.4116475858636137\n",
      "Recall:  0.3724386399459581\n",
      "F1 Score:  0.3910627733774678\n",
      "---\n",
      "\n",
      " Training: Epoch 12 - Batch 124/125: Loss: 0.1868 ---\n",
      "Precision:  0.4496482784154017\n",
      "Recall:  0.3895541537474607\n",
      "F1 Score:  0.41744958753437217\n",
      "---\n",
      "\n",
      " Dev: Epoch 12 - Batch 249/250: Loss: 0.1892\n",
      "Epoch dev loss: 47.294508673250675\n",
      "Previous epoch dev loss: 47.462187692523\n",
      "\n",
      "---\n",
      "Precision:  0.44375311100049775\n",
      "Recall:  0.3611504962527851\n",
      "F1 Score:  0.39821328866555\n",
      "---\n",
      "\n",
      " Training: Epoch 13 - Batch 124/125: Loss: 0.1859 ---\n",
      "Precision:  0.46279155868196964\n",
      "Recall:  0.39213635888319565\n",
      "F1 Score:  0.42454432242726137\n",
      "---\n",
      "\n",
      " Dev: Epoch 13 - Batch 249/250: Loss: 0.1893\n",
      "Epoch dev loss: 47.32430633157492\n",
      "Previous epoch dev loss: 47.294508673250675\n",
      "\n",
      "---\n",
      "Precision:  0.4116475858636137\n",
      "Recall:  0.38518863530507685\n",
      "F1 Score:  0.39797882579403276\n",
      "---\n",
      "\n",
      "early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--- set up ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('using gpu')\n",
    "else:\n",
    "    print('using cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# OPTIMIZERS & REGURALIZATION\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.5)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "#--- TRAINING ---\n",
    "\n",
    "previous_train_loss = 1000000\n",
    "previous_dev_loss = 1000000\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    targets = []\n",
    "    \n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "                \n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        preds.extend((pred > 0.25).float().cpu().numpy())\n",
    "        targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        #total += len(target)\n",
    "        #train_correct += torch.sum((pred > 0.25).float() == target) / torch.sum(target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('\\r', 'Training: Epoch %d - Batch %d/%d: Loss: %.4f ' % \n",
    "              (epoch, batch_num, len(train_loader), train_loss / (batch_num + 1)), end='')\n",
    "        \n",
    "    \n",
    "    precision, recall, f1_score = calc_batch_metrics(targets, preds)\n",
    "    print('---')\n",
    "    print('Precision: ' , precision)\n",
    "    print('Recall: ' , recall) \n",
    "    print('F1 Score: ' , f1_score)\n",
    "    print('---')\n",
    "    \n",
    "    print()\n",
    "    dev_loss = 0\n",
    "    dev_total = 0\n",
    "    dev_correct = 0\n",
    "    \n",
    "    dev_preds = []\n",
    "    dev_targets = []\n",
    "    \n",
    "    for batch_num, (data, target) in enumerate(dev_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "               \n",
    "        # Compute prediction error\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "        dev_loss += loss.item()\n",
    "       \n",
    "        #dev_total += len(target)\n",
    "        #dev_correct += torch.sum((pred > 0.25).float() == target) / torch.sum(target)\n",
    "    \n",
    "        dev_preds.extend((pred > 0.25).float().cpu().numpy())\n",
    "        dev_targets.extend(target.cpu().numpy())\n",
    "       \n",
    "        print('\\r', 'Dev: Epoch %d - Batch %d/%d: Loss: %.4f' % \n",
    "             (epoch, batch_num, len(dev_loader), dev_loss / (batch_num + 1)), end='')\n",
    "    \n",
    "    print()\n",
    "    print(\"Epoch dev loss: \" + str(dev_loss))\n",
    "    print(\"Previous epoch dev loss: \" + str(previous_dev_loss))\n",
    "    print()\n",
    "    \n",
    "    dprecision, drecall, df1_score = calc_batch_metrics(dev_targets, dev_preds)\n",
    "    print('---')\n",
    "    print('Precision: ' , dprecision)\n",
    "    print('Recall: ' , drecall) \n",
    "    print('F1 Score: ' , df1_score)\n",
    "    print('---')\n",
    "    print()\n",
    "    \n",
    "    # EARLY STOPPING\n",
    "    if dev_loss > previous_dev_loss:\n",
    "        print('early stopping')\n",
    "        break\n",
    "        \n",
    "    previous_dev_loss = dev_loss\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(batch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 0/128: Loss: 0.2954 | Test Acc: 35.846% (5/16)\n",
      "Evaluating: Batch 1/128: Loss: 0.3101 | Test Acc: 37.819% (12/32)\n",
      "Evaluating: Batch 2/128: Loss: 0.2932 | Test Acc: 38.685% (18/48)\n",
      "Evaluating: Batch 3/128: Loss: 0.2873 | Test Acc: 38.584% (24/64)\n",
      "Evaluating: Batch 4/128: Loss: 0.2966 | Test Acc: 39.100% (31/80)\n",
      "Evaluating: Batch 5/128: Loss: 0.2946 | Test Acc: 37.874% (36/96)\n",
      "Evaluating: Batch 6/128: Loss: 0.2947 | Test Acc: 38.071% (42/112)\n",
      "Evaluating: Batch 7/128: Loss: 0.2886 | Test Acc: 38.195% (48/128)\n",
      "Evaluating: Batch 8/128: Loss: 0.2868 | Test Acc: 38.645% (55/144)\n",
      "Evaluating: Batch 9/128: Loss: 0.2841 | Test Acc: 38.752% (62/160)\n",
      "Evaluating: Batch 10/128: Loss: 0.2804 | Test Acc: 39.036% (68/176)\n",
      "Evaluating: Batch 11/128: Loss: 0.2804 | Test Acc: 39.559% (75/192)\n",
      "Evaluating: Batch 12/128: Loss: 0.2821 | Test Acc: 39.050% (81/208)\n",
      "Evaluating: Batch 13/128: Loss: 0.2827 | Test Acc: 38.980% (87/224)\n",
      "Evaluating: Batch 14/128: Loss: 0.2792 | Test Acc: 39.123% (93/240)\n",
      "Evaluating: Batch 15/128: Loss: 0.2807 | Test Acc: 39.160% (100/256)\n",
      "Evaluating: Batch 16/128: Loss: 0.2805 | Test Acc: 38.905% (105/272)\n",
      "Evaluating: Batch 17/128: Loss: 0.2804 | Test Acc: 38.533% (110/288)\n",
      "Evaluating: Batch 18/128: Loss: 0.2807 | Test Acc: 38.808% (117/304)\n",
      "Evaluating: Batch 19/128: Loss: 0.2805 | Test Acc: 38.951% (124/320)\n",
      "Evaluating: Batch 20/128: Loss: 0.2811 | Test Acc: 38.759% (130/336)\n",
      "Evaluating: Batch 21/128: Loss: 0.2796 | Test Acc: 38.660% (136/352)\n",
      "Evaluating: Batch 22/128: Loss: 0.2777 | Test Acc: 38.586% (141/368)\n",
      "Evaluating: Batch 23/128: Loss: 0.2787 | Test Acc: 38.608% (148/384)\n",
      "Evaluating: Batch 24/128: Loss: 0.2770 | Test Acc: 38.527% (154/400)\n",
      "Evaluating: Batch 25/128: Loss: 0.2771 | Test Acc: 38.790% (161/416)\n",
      "Evaluating: Batch 26/128: Loss: 0.2766 | Test Acc: 39.107% (168/432)\n",
      "Evaluating: Batch 27/128: Loss: 0.2754 | Test Acc: 39.191% (175/448)\n",
      "Evaluating: Batch 28/128: Loss: 0.2761 | Test Acc: 39.262% (182/464)\n",
      "Evaluating: Batch 29/128: Loss: 0.2763 | Test Acc: 38.921% (186/480)\n",
      "Evaluating: Batch 30/128: Loss: 0.2771 | Test Acc: 39.166% (194/496)\n",
      "Evaluating: Batch 31/128: Loss: 0.2786 | Test Acc: 39.144% (200/512)\n",
      "Evaluating: Batch 32/128: Loss: 0.2783 | Test Acc: 38.956% (205/528)\n",
      "Evaluating: Batch 33/128: Loss: 0.2782 | Test Acc: 39.159% (213/544)\n",
      "Evaluating: Batch 34/128: Loss: 0.2794 | Test Acc: 39.153% (219/560)\n",
      "Evaluating: Batch 35/128: Loss: 0.2769 | Test Acc: 39.077% (225/576)\n",
      "Evaluating: Batch 36/128: Loss: 0.2778 | Test Acc: 39.167% (231/592)\n",
      "Evaluating: Batch 37/128: Loss: 0.2792 | Test Acc: 39.248% (238/608)\n",
      "Evaluating: Batch 38/128: Loss: 0.2779 | Test Acc: 39.180% (244/624)\n",
      "Evaluating: Batch 39/128: Loss: 0.2788 | Test Acc: 39.119% (250/640)\n",
      "Evaluating: Batch 40/128: Loss: 0.2788 | Test Acc: 39.119% (256/656)\n",
      "Evaluating: Batch 41/128: Loss: 0.2787 | Test Acc: 39.165% (263/672)\n",
      "Evaluating: Batch 42/128: Loss: 0.2787 | Test Acc: 39.298% (270/688)\n",
      "Evaluating: Batch 43/128: Loss: 0.2779 | Test Acc: 39.165% (275/704)\n",
      "Evaluating: Batch 44/128: Loss: 0.2775 | Test Acc: 39.281% (282/720)\n",
      "Evaluating: Batch 45/128: Loss: 0.2759 | Test Acc: 39.157% (288/736)\n",
      "Evaluating: Batch 46/128: Loss: 0.2756 | Test Acc: 39.057% (293/752)\n",
      "Evaluating: Batch 47/128: Loss: 0.2754 | Test Acc: 39.220% (301/768)\n",
      "Evaluating: Batch 48/128: Loss: 0.2766 | Test Acc: 39.173% (307/784)\n",
      "Evaluating: Batch 49/128: Loss: 0.2763 | Test Acc: 39.018% (312/800)\n",
      "Evaluating: Batch 50/128: Loss: 0.2765 | Test Acc: 39.093% (319/816)\n",
      "Evaluating: Batch 51/128: Loss: 0.2763 | Test Acc: 39.223% (326/832)\n",
      "Evaluating: Batch 52/128: Loss: 0.2767 | Test Acc: 39.300% (333/848)\n",
      "Evaluating: Batch 53/128: Loss: 0.2761 | Test Acc: 39.399% (340/864)\n",
      "Evaluating: Batch 54/128: Loss: 0.2759 | Test Acc: 39.423% (346/880)\n",
      "Evaluating: Batch 55/128: Loss: 0.2764 | Test Acc: 39.525% (354/896)\n",
      "Evaluating: Batch 56/128: Loss: 0.2763 | Test Acc: 39.479% (360/912)\n",
      "Evaluating: Batch 57/128: Loss: 0.2763 | Test Acc: 39.506% (366/928)\n",
      "Evaluating: Batch 58/128: Loss: 0.2768 | Test Acc: 39.518% (373/944)\n",
      "Evaluating: Batch 59/128: Loss: 0.2761 | Test Acc: 39.596% (380/960)\n",
      "Evaluating: Batch 60/128: Loss: 0.2758 | Test Acc: 39.691% (387/976)\n",
      "Evaluating: Batch 61/128: Loss: 0.2752 | Test Acc: 39.624% (393/992)\n",
      "Evaluating: Batch 62/128: Loss: 0.2753 | Test Acc: 39.635% (399/1008)\n",
      "Evaluating: Batch 63/128: Loss: 0.2750 | Test Acc: 39.563% (405/1024)\n",
      "Evaluating: Batch 64/128: Loss: 0.2749 | Test Acc: 39.652% (412/1040)\n",
      "Evaluating: Batch 65/128: Loss: 0.2741 | Test Acc: 39.658% (418/1056)\n",
      "Evaluating: Batch 66/128: Loss: 0.2734 | Test Acc: 39.478% (423/1072)\n",
      "Evaluating: Batch 67/128: Loss: 0.2736 | Test Acc: 39.467% (429/1088)\n",
      "Evaluating: Batch 68/128: Loss: 0.2729 | Test Acc: 39.461% (435/1104)\n",
      "Evaluating: Batch 69/128: Loss: 0.2726 | Test Acc: 39.436% (441/1120)\n",
      "Evaluating: Batch 70/128: Loss: 0.2723 | Test Acc: 39.417% (447/1136)\n",
      "Evaluating: Batch 71/128: Loss: 0.2731 | Test Acc: 39.435% (454/1152)\n",
      "Evaluating: Batch 72/128: Loss: 0.2724 | Test Acc: 39.529% (461/1168)\n",
      "Evaluating: Batch 73/128: Loss: 0.2723 | Test Acc: 39.525% (467/1184)\n",
      "Evaluating: Batch 74/128: Loss: 0.2717 | Test Acc: 39.463% (473/1200)\n",
      "Evaluating: Batch 75/128: Loss: 0.2720 | Test Acc: 39.448% (479/1216)\n",
      "Evaluating: Batch 76/128: Loss: 0.2716 | Test Acc: 39.477% (486/1232)\n",
      "Evaluating: Batch 77/128: Loss: 0.2725 | Test Acc: 39.592% (494/1248)\n",
      "Evaluating: Batch 78/128: Loss: 0.2722 | Test Acc: 39.576% (500/1264)\n",
      "Evaluating: Batch 79/128: Loss: 0.2727 | Test Acc: 39.572% (506/1280)\n",
      "Evaluating: Batch 80/128: Loss: 0.2725 | Test Acc: 39.638% (513/1296)\n",
      "Evaluating: Batch 81/128: Loss: 0.2729 | Test Acc: 39.607% (519/1312)\n",
      "Evaluating: Batch 82/128: Loss: 0.2726 | Test Acc: 39.649% (526/1328)\n",
      "Evaluating: Batch 83/128: Loss: 0.2732 | Test Acc: 39.640% (532/1344)\n",
      "Evaluating: Batch 84/128: Loss: 0.2728 | Test Acc: 39.584% (538/1360)\n",
      "Evaluating: Batch 85/128: Loss: 0.2731 | Test Acc: 39.623% (545/1376)\n",
      "Evaluating: Batch 86/128: Loss: 0.2742 | Test Acc: 39.542% (550/1392)\n",
      "Evaluating: Batch 87/128: Loss: 0.2743 | Test Acc: 39.636% (558/1408)\n",
      "Evaluating: Batch 88/128: Loss: 0.2739 | Test Acc: 39.617% (564/1424)\n",
      "Evaluating: Batch 89/128: Loss: 0.2740 | Test Acc: 39.724% (572/1440)\n",
      "Evaluating: Batch 90/128: Loss: 0.2736 | Test Acc: 39.718% (578/1456)\n",
      "Evaluating: Batch 91/128: Loss: 0.2735 | Test Acc: 39.690% (584/1472)\n",
      "Evaluating: Batch 92/128: Loss: 0.2733 | Test Acc: 39.679% (590/1488)\n",
      "Evaluating: Batch 93/128: Loss: 0.2731 | Test Acc: 39.734% (597/1504)\n",
      "Evaluating: Batch 94/128: Loss: 0.2726 | Test Acc: 39.676% (603/1520)\n",
      "Evaluating: Batch 95/128: Loss: 0.2729 | Test Acc: 39.595% (608/1536)\n",
      "Evaluating: Batch 96/128: Loss: 0.2732 | Test Acc: 39.620% (614/1552)\n",
      "Evaluating: Batch 97/128: Loss: 0.2740 | Test Acc: 39.579% (620/1568)\n",
      "Evaluating: Batch 98/128: Loss: 0.2737 | Test Acc: 39.564% (626/1584)\n",
      "Evaluating: Batch 99/128: Loss: 0.2739 | Test Acc: 39.517% (632/1600)\n",
      "Evaluating: Batch 100/128: Loss: 0.2738 | Test Acc: 39.530% (638/1616)\n",
      "Evaluating: Batch 101/128: Loss: 0.2738 | Test Acc: 39.541% (645/1632)\n",
      "Evaluating: Batch 102/128: Loss: 0.2738 | Test Acc: 39.555% (651/1648)\n",
      "Evaluating: Batch 103/128: Loss: 0.2735 | Test Acc: 39.565% (658/1664)\n",
      "Evaluating: Batch 104/128: Loss: 0.2732 | Test Acc: 39.596% (665/1680)\n",
      "Evaluating: Batch 105/128: Loss: 0.2738 | Test Acc: 39.644% (672/1696)\n",
      "Evaluating: Batch 106/128: Loss: 0.2732 | Test Acc: 39.679% (679/1712)\n",
      "Evaluating: Batch 107/128: Loss: 0.2734 | Test Acc: 39.698% (685/1728)\n",
      "Evaluating: Batch 108/128: Loss: 0.2732 | Test Acc: 39.717% (692/1744)\n",
      "Evaluating: Batch 109/128: Loss: 0.2732 | Test Acc: 39.635% (697/1760)\n",
      "Evaluating: Batch 110/128: Loss: 0.2732 | Test Acc: 39.611% (703/1776)\n",
      "Evaluating: Batch 111/128: Loss: 0.2731 | Test Acc: 39.542% (708/1792)\n",
      "Evaluating: Batch 112/128: Loss: 0.2727 | Test Acc: 39.500% (714/1808)\n",
      "Evaluating: Batch 113/128: Loss: 0.2729 | Test Acc: 39.434% (719/1824)\n",
      "Evaluating: Batch 114/128: Loss: 0.2735 | Test Acc: 39.561% (727/1840)\n",
      "Evaluating: Batch 115/128: Loss: 0.2734 | Test Acc: 39.575% (734/1856)\n",
      "Evaluating: Batch 116/128: Loss: 0.2730 | Test Acc: 39.584% (741/1872)\n",
      "Evaluating: Batch 117/128: Loss: 0.2728 | Test Acc: 39.540% (746/1888)\n",
      "Evaluating: Batch 118/128: Loss: 0.2729 | Test Acc: 39.570% (753/1904)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 119/128: Loss: 0.2733 | Test Acc: 39.642% (761/1920)\n",
      "Evaluating: Batch 120/128: Loss: 0.2738 | Test Acc: 39.676% (768/1936)\n",
      "Evaluating: Batch 121/128: Loss: 0.2735 | Test Acc: 39.672% (774/1952)\n",
      "Evaluating: Batch 122/128: Loss: 0.2736 | Test Acc: 39.618% (779/1968)\n",
      "Evaluating: Batch 123/128: Loss: 0.2733 | Test Acc: 39.617% (786/1984)\n",
      "Evaluating: Batch 124/128: Loss: 0.2731 | Test Acc: 39.615% (792/2000)\n",
      "Evaluating: Batch 125/128: Loss: 0.2735 | Test Acc: 39.622% (798/2016)\n",
      "Evaluating: Batch 126/128: Loss: 0.2734 | Test Acc: 39.681% (806/2032)\n",
      "Evaluating: Batch 127/128: Loss: 0.2728 | Test Acc: 39.854% (811/2036)\n",
      "AVG test precision:0.5528336726490888\n",
      "AVG test recall:0.5768638230102818\n",
      "AVG test f1 score:0.5614790327882354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--- train ---\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "test_f1_score = []\n",
    "\n",
    "test_acc_all = 0\n",
    "\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(dev_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        test_total += len(target)\n",
    "        test_correct += torch.sum((pred > 0.5).float() == target) / torch.sum(target)\n",
    "        preds.extend(pred.argmax(1).cpu().numpy())\n",
    "        targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        test_acc_all += (test_correct / total)\n",
    "        \n",
    "        precision, recall, f1_score = calc_batch_metrics(target, pred)\n",
    "        \n",
    "        test_precision.append(precision)\n",
    "        test_recall.append(recall)\n",
    "        test_f1_score.append(f1_score)\n",
    "\n",
    "\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num, len(dev_loader), test_loss / (batch_num + 1), \n",
    "               100. * test_correct / test_total, test_correct, test_total))\n",
    "    \n",
    "    #cf_matrix = confusion_matrix(targets, preds)\n",
    "    #df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix) * 1000, \n",
    "    #                     index = [i for i in annotation_dict_train.keys()],\n",
    "    #                     columns=[i for i in annotation_dict_train.keys()])\n",
    "    #plt.figure(figsize=(12, 7))\n",
    "    #sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "    \n",
    "# print('AVG test acc:' + str(test_acc_all*100/len(dev_loader)) + '%')\n",
    "print('AVG test precision:' + str(np.mean(test_precision)))\n",
    "print('AVG test recall:' + str(np.mean(test_recall)))\n",
    "print('AVG test f1 score:' + str(np.mean(test_f1_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
