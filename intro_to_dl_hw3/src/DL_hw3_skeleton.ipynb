{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "   Introduction to Deep Learning\n",
    "   Assignment 3: Sentiment Classification of Tweets on a Recurrent Neural Network using Pretrained Embeddings\n",
    "\n",
    "   Hande Celikkanat\n",
    "\n",
    "   Credit: Data preparation pipeline adopted from https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import spacy\n",
    "import regex as re\n",
    "from torchtext import vocab\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants - Add here as you wish\n",
    "N_EPOCHS = 5\n",
    "EMBEDDING_DIM = 200\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilary functions for data preparation\n",
    "tok = spacy.load('en_core_web_sm',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(s):\n",
    "    return [w.text.lower() for w in tok(tweet_clean(s))]\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def get_accuracy(output, gold):\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "    correct = torch.sum(torch.eq(predicted,gold)).item()\n",
    "    acc = correct / gold.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.TweetText\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            acc = get_accuracy(predictions, batch.Label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Network\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = EMBEDDING_DIM\n",
    "        self.n_hidden = 64\n",
    "        self.n_out = OUTPUT_DIM\n",
    "        bidirectional = False\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        #self.embedding.weight.data.copy_(train_data.fields['TweetText'].vocab.vectors)\n",
    "        self.embedding.weight.requires_grad = False # make embedding non trainable\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.n_hidden, bidirectional=bidirectional)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.out = nn.Linear(self.n_hidden*2*2, self.n_out)\n",
    "        else:\n",
    "            self.out = nn.Linear(self.n_hidden*2, self.n_out)\n",
    "\n",
    "    def forward(self, x, text_lengths):\n",
    "        bs = x.size(1)\n",
    "        #self.h = self.init_hidden(bs)\n",
    "        x = x.transpose(0,1)\n",
    "        embs = self.embedding(x)\n",
    "        embs = embs.transpose(0,1)\n",
    "        embs = nn.utils.rnn.pack_padded_sequence(embs, text_lengths.cpu())\n",
    "        gru_out, self.h = self.gru(embs)\n",
    "        gru_out, lengths = nn.utils.rnn.pad_packed_sequence(gru_out)        \n",
    "        \n",
    "        avg_pool = F.adaptive_avg_pool1d(gru_out.permute(1,2,0),1).view(bs,-1)\n",
    "        max_pool = F.adaptive_max_pool1d(gru_out.permute(1,2,0),1).view(bs,-1)        \n",
    "        # outp = self.out(torch.cat([self.h[-1],avg_pool,max_pool],dim=1))\n",
    "        outp = self.out(torch.cat([avg_pool,max_pool],dim=1))\n",
    "        return F.log_softmax(outp, dim=-1)\n",
    "    \n",
    "    def init_hidden(self, batch_size): \n",
    "        if self.bidirectional:\n",
    "            return torch.zeros((2,batch_size,self.n_hidden)).to(device)\n",
    "        else:\n",
    "            return torch.zeros((1,batch_size,self.n_hidden)).to(device)\n",
    "            # return torch.zeros((1,batch_size,self.n_hidden)).cuda().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Data Preparation ---\n",
    "\n",
    "# define the columns that we want to process and how to process\n",
    "txt_field = torchtext.data.Field(sequential=True,\n",
    "                                 tokenize=tokenizer,\n",
    "                                 include_lengths=True,\n",
    "                                 use_vocab=True)\n",
    "label_field = torchtext.data.Field(sequential=False,\n",
    "                                   use_vocab=False)\n",
    "\n",
    "csv_fields = [\n",
    "    ('Label', label_field), # process this field as the class label\n",
    "    ('TweetID', None), # we dont need this field\n",
    "    ('Timestamp', None), # we dont need this field\n",
    "    ('Flag', None), # we dont need this field\n",
    "    ('UseerID', None), # we dont need this field\n",
    "    ('TweetText', txt_field) # process it as text field\n",
    "]\n",
    "\n",
    "train_data, dev_data, test_data = torchtext.data.TabularDataset.splits(path='../data',\n",
    "                                                                       format='csv',\n",
    "                                                                       train='sent140.train.mini.csv',\n",
    "                                                                       validation='sent140.dev.csv',\n",
    "                                                                       test='sent140.test.csv',\n",
    "                                                                       fields=csv_fields,\n",
    "                                                                       skip_header=False)\n",
    "\n",
    "\n",
    "txt_field.build_vocab(train_data, dev_data, max_size=100000,\n",
    "                      vectors='glove.twitter.27B.200d', unk_init = torch.Tensor.normal_)\n",
    "label_field.build_vocab(train_data)\n",
    "\n",
    "train_iter, dev_iter, test_iter = torchtext.data.BucketIterator.splits(datasets=(train_data, dev_data, test_data),\n",
    "                                            batch_sizes=(50,50,50),  # batch sizes of train, dev, test\n",
    "                                            sort_key=lambda x: len(x.TweetText), # how to sort text\n",
    "                                            device=device,\n",
    "                                            sort_within_batch=True,\n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model, Loss, Optimizer Initialization ---\n",
    "\n",
    "PAD_IDX = txt_field.vocab.stoi[txt_field.pad_token]\n",
    "UNK_IDX = txt_field.vocab.stoi[txt_field.unk_token]\n",
    "\n",
    "# WRITE CODE HERE\n",
    "model = RNN(len(txt_field.vocab.vectors))\n",
    "\n",
    "# Copy the pretrained embeddings into the model\n",
    "pretrained_embeddings = txt_field.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Fix the <UNK> and <PAD> tokens in the embedding layer\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "# WRITE CODE HERE\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.588 | Train Acc: 68.45%\n",
      "\t Val. Loss: 0.528 |  Val. Acc: 74.28%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.497 | Train Acc: 75.69%\n",
      "\t Val. Loss: 0.502 |  Val. Acc: 75.49%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.456 | Train Acc: 78.51%\n",
      "\t Val. Loss: 0.489 |  Val. Acc: 76.27%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.420 | Train Acc: 81.34%\n",
      "\t Val. Loss: 0.503 |  Val. Acc: 76.19%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.377 | Train Acc: 83.96%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 76.60%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train Loop ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iter:\n",
    "        text, text_lengths = batch.TweetText\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.Label)\n",
    "        acc = get_accuracy(predictions, batch.Label)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_acc = (epoch_loss / len(train_iter), epoch_acc / len(train_iter))\n",
    "    valid_loss, valid_acc = evaluate(model, dev_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
