{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cX61QTz_WACp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   Introduction to Deep Learning (LDA-T3114)\\n   Skeleton Code for Assignment 1: Sentiment Classification on a Feed-Forward Neural Network\\n\\n   Hande Celikkanat & Miikka Silfverberg\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "   Introduction to Deep Learning (LDA-T3114)\n",
    "   Skeleton Code for Assignment 1: Sentiment Classification on a Feed-Forward Neural Network\n",
    "\n",
    "   Hande Celikkanat & Miikka Silfverberg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTgWmDBDWutR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda available True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#ATTENTION: If necessary, add the paths to your data_semeval.py and paths.py here:\n",
    "#import sys\n",
    "#sys.path.append('</path/to/below/modules>')\n",
    "from data_semeval import *\n",
    "from paths import data_dir\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Is cuda available', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJp3BkaHWw74"
   },
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "\n",
    "N_CLASSES = len(LABEL_INDICES)\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 0.25\n",
    "BATCH_SIZE = 100\n",
    "REPORT_EVERY = 1\n",
    "IS_VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gvAm0fHW2lH"
   },
   "outputs": [],
   "source": [
    "def make_bow(tweet, indices):\n",
    "    feature_ids = list(indices[tok] for tok in tweet['BODY'] if tok in indices)\n",
    "    bow_vec = torch.zeros(len(indices))\n",
    "    bow_vec[feature_ids] = 1\n",
    "    return bow_vec.view(1, -1)\n",
    "\n",
    "def generate_bow_representations(data):\n",
    "    vocab = set(token for tweet in data['training'] for token in tweet['BODY'])\n",
    "    vocab_size = len(vocab) \n",
    "    indices = {w:i for i, w in enumerate(vocab)}\n",
    "  \n",
    "    for split in [\"training\",\"development.input\",\"development.gold\",\n",
    "                  \"test.input\",\"test.gold\"]:\n",
    "        for tweet in data[split]:\n",
    "            tweet['BOW'] = make_bow(tweet,indices)\n",
    "\n",
    "    return indices, vocab_size\n",
    "\n",
    "# Convert string label to pytorch format.\n",
    "def label_to_idx(label):\n",
    "    return torch.LongTensor([LABEL_INDICES[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQU98XHwW50h"
   },
   "outputs": [],
   "source": [
    "#--- model ---\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    # Feel free to add whichever arguments you like here.\n",
    "    def __init__(self, vocab_size, n_classes, extra_arg_1=None, extra_arg_2=None):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(vocab_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiXMR3LAW_GC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22031\n"
     ]
    }
   ],
   "source": [
    "#--- data loading ---\n",
    "data = read_semeval_datasets(data_dir)\n",
    "indices, vocab_size = generate_bow_representations(data)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0k4wWf8XCMr"
   },
   "outputs": [],
   "source": [
    "#--- set up ---\n",
    "\n",
    "# WRITE CODE HERE\n",
    "model = FFNN(vocab_size, N_CLASSES).to(device) #add extra arguments here if you use\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAq0EdZIXJo5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.9937\n",
      "epoch: 2, loss: 0.9168\n",
      "epoch: 3, loss: 0.8224\n",
      "epoch: 4, loss: 0.7534\n",
      "epoch: 5, loss: 0.6922\n",
      "epoch: 6, loss: 0.6481\n",
      "epoch: 7, loss: 0.5787\n",
      "epoch: 8, loss: 0.5448\n",
      "epoch: 9, loss: 0.4951\n",
      "epoch: 10, loss: 0.4528\n",
      "epoch: 11, loss: 0.3289\n",
      "epoch: 12, loss: 0.4159\n",
      "epoch: 13, loss: 0.3026\n",
      "epoch: 14, loss: 0.2232\n",
      "epoch: 15, loss: 0.0901\n",
      "epoch: 16, loss: 0.0390\n",
      "epoch: 17, loss: 0.0278\n",
      "epoch: 18, loss: 0.0229\n",
      "epoch: 19, loss: 0.0186\n",
      "epoch: 20, loss: 0.0140\n",
      "epoch: 21, loss: 0.0132\n",
      "epoch: 22, loss: 0.0117\n",
      "epoch: 23, loss: 0.0127\n",
      "epoch: 24, loss: 0.0101\n",
      "epoch: 25, loss: 0.0076\n",
      "epoch: 26, loss: 0.0104\n",
      "epoch: 27, loss: 0.0080\n",
      "epoch: 28, loss: 0.0095\n",
      "epoch: 29, loss: 0.0082\n",
      "epoch: 30, loss: 0.0074\n",
      "epoch: 31, loss: 0.0088\n",
      "epoch: 32, loss: 0.0080\n",
      "epoch: 33, loss: 0.0070\n",
      "epoch: 34, loss: 0.0067\n",
      "epoch: 35, loss: 0.0074\n",
      "epoch: 36, loss: 0.0059\n",
      "epoch: 37, loss: 0.0074\n",
      "epoch: 38, loss: 0.0070\n",
      "epoch: 39, loss: 0.0063\n",
      "epoch: 40, loss: 0.0068\n",
      "epoch: 41, loss: 0.0059\n",
      "epoch: 42, loss: 0.0069\n",
      "epoch: 43, loss: 0.0062\n",
      "epoch: 44, loss: 0.0045\n",
      "epoch: 45, loss: 0.0054\n",
      "epoch: 46, loss: 0.0043\n",
      "epoch: 47, loss: 0.0055\n",
      "epoch: 48, loss: 0.0051\n",
      "epoch: 49, loss: 0.0053\n",
      "epoch: 50, loss: 0.0055\n",
      "epoch: 51, loss: 0.0052\n",
      "epoch: 52, loss: 0.0057\n",
      "epoch: 53, loss: 0.0040\n",
      "epoch: 54, loss: 0.0044\n",
      "epoch: 55, loss: 0.0051\n",
      "epoch: 56, loss: 0.0050\n",
      "epoch: 57, loss: 0.0044\n",
      "epoch: 58, loss: 0.0040\n",
      "epoch: 59, loss: 0.0046\n",
      "epoch: 60, loss: 0.0040\n",
      "epoch: 61, loss: 0.0045\n",
      "epoch: 62, loss: 0.0040\n",
      "epoch: 63, loss: 0.0040\n",
      "epoch: 64, loss: 0.0047\n",
      "epoch: 65, loss: 0.0037\n",
      "epoch: 66, loss: 0.0042\n",
      "epoch: 67, loss: 0.0037\n",
      "epoch: 68, loss: 0.0042\n",
      "epoch: 69, loss: 0.0036\n",
      "epoch: 70, loss: 0.0034\n",
      "epoch: 71, loss: 0.0041\n",
      "epoch: 72, loss: 0.0043\n",
      "epoch: 73, loss: 0.0041\n",
      "epoch: 74, loss: 0.0039\n",
      "epoch: 75, loss: 0.0040\n",
      "epoch: 76, loss: 0.0036\n",
      "epoch: 77, loss: 0.0041\n",
      "epoch: 78, loss: 0.0036\n",
      "epoch: 79, loss: 0.0036\n",
      "epoch: 80, loss: 0.0041\n",
      "epoch: 81, loss: 0.0031\n",
      "epoch: 82, loss: 0.0036\n",
      "epoch: 83, loss: 0.0034\n",
      "epoch: 84, loss: 0.0035\n",
      "epoch: 85, loss: 0.0033\n",
      "epoch: 86, loss: 0.0036\n",
      "epoch: 87, loss: 0.0038\n",
      "epoch: 88, loss: 0.0034\n",
      "epoch: 89, loss: 0.0038\n",
      "epoch: 90, loss: 0.0037\n",
      "epoch: 91, loss: 0.0039\n",
      "epoch: 92, loss: 0.0035\n",
      "epoch: 93, loss: 0.0031\n",
      "epoch: 94, loss: 0.0037\n",
      "epoch: 95, loss: 0.0031\n",
      "epoch: 96, loss: 0.0036\n",
      "epoch: 97, loss: 0.0030\n",
      "epoch: 98, loss: 0.0032\n",
      "epoch: 99, loss: 0.0037\n",
      "epoch: 100, loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.train()\n",
    "\n",
    "def label_to_tensor(label):\n",
    "    return torch.as_tensor([float(int(label_to_idx(label) == i)) for i in range(N_CLASSES)])\n",
    "\n",
    "#--- training ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0\n",
    "    # Generally speaking, it's a good idea to shuffle your\n",
    "    # datasets once every epoch.\n",
    "    random.shuffle(data['training'])    \n",
    "\n",
    "    for i in range(int(len(data['training'])/BATCH_SIZE)):\n",
    "        minibatch = data['training'][i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        X = torch.stack([x['BOW'][0] for x in minibatch]).to(device)\n",
    "        y = torch.stack([label_to_tensor(y['SENTIMENT']) for y in minibatch]).to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    if ((epoch+1) % REPORT_EVERY) == 0:\n",
    "        print('epoch: %d, loss: %.4f' % (epoch+1, total_loss*BATCH_SIZE/len(data['training'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47CYoN-RXLfl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 66.65\n"
     ]
    }
   ],
   "source": [
    "#--- test ---\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for tweet in data['test.gold']:\n",
    "        gold_class = label_to_idx(tweet['SENTIMENT'])\n",
    "\n",
    "        gold_class = label_to_idx(tweet['SENTIMENT'])\n",
    "\n",
    "        bow = tweet['BOW'].to(device)\n",
    "        predicted = model(bow)\n",
    "        \n",
    "        if int(predicted.argmax()) == int(gold_class):\n",
    "            correct += 1\n",
    "        \n",
    "        #if IS_VERBOSE:\n",
    "        #    print('TEST DATA: %s, GOLD LABEL: %s, GOLD CLASS %d, OUTPUT: %d' % \n",
    "        #         (' '.join(tweet['BODY'][:-1]), tweet['SENTIMENT'], gold_class, predicted))\n",
    "\n",
    "    print('test accuracy: %.2f' % (100.0 * correct / len(data['test.gold'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL_hw1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
